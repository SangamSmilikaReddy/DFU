{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe662feb-0dbe-4128-a418-2b3cba476185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting Pillow\n",
      "  Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting keras_tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (3.0.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (2.31.0)\n",
      "Collecting kt-legacy (from keras_tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (2.1.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.0.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (3.10.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.1.8)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: pytz, kt-legacy, tzdata, threadpoolctl, scipy, Pillow, joblib, scikit-learn, pandas, keras_tuner\n",
      "Successfully installed Pillow-11.0.0 joblib-1.4.2 keras_tuner-1.4.7 kt-legacy-1.0.5 pandas-2.2.3 pytz-2024.2 scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0 tzdata-2024.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn numpy Pillow keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3ab47e-6715-4964-9c81-a76056afa2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ischaemia Class Mapping:\n",
      "non-ischemia: 1\n",
      "ischemia: 0\n",
      "Infection Class Mapping:\n",
      "non-infection: 1\n",
      "infection: 0\n",
      "Shape of Ischaemia images array: (9870, 256, 256, 3)\n",
      "Shape of Ischaemia target labels array: (9870,)\n",
      "Shape of Infection images array: (5890, 256, 256, 3)\n",
      "Shape of Infection target labels array: (5890,)\n",
      "Ischaemia Training set shape: (6909, 256, 256, 3) (6909,)\n",
      "Ischaemia Validation set shape: (2220, 256, 256, 3) (2220,)\n",
      "Ischaemia Test set shape: (741, 256, 256, 3) (741,)\n",
      "Infection Training set shape: (4123, 256, 256, 3) (4123,)\n",
      "Infection Validation set shape: (1325, 256, 256, 3) (1325,)\n",
      "Infection Test set shape: (442, 256, 256, 3) (442,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the root directory where your image folders are located\n",
    "root_directory = \"PartB_DFU_dataset - Copy\"\n",
    "\n",
    "# Initialize lists to store image paths and corresponding class labels for both datasets\n",
    "image_paths_ischaemia = []\n",
    "categories_ischaemia = []\n",
    "image_paths_infection = []\n",
    "categories_infection = []\n",
    "\n",
    "# Iterate over each class and its subdirectories\n",
    "for class_name in [\"Infection\", \"Ischaemia\"]:\n",
    "    for augmentation_type in [\"Aug-Negative\", \"Aug-Positive\"]:\n",
    "        folder_path = os.path.join(root_directory, class_name, augmentation_type)\n",
    "        category = f\"{class_name.lower()}{'pov' if 'Positive' in augmentation_type else 'neg'}\"\n",
    "        \n",
    "        # Iterate over image files in the current directory\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".jpg\"):  # Assuming images are jpg format\n",
    "                image_path = os.path.join(folder_path, file_name)\n",
    "                if class_name == \"Ischaemia\":\n",
    "                    image_paths_ischaemia.append(image_path)\n",
    "                    categories_ischaemia.append(\"ischemia\" if \"Positive\" in augmentation_type else \"non-ischemia\")\n",
    "                elif class_name == \"Infection\":\n",
    "                    image_paths_infection.append(image_path)\n",
    "                    categories_infection.append(\"infection\" if \"Positive\" in augmentation_type else \"non-infection\")\n",
    "\n",
    "# Create DataFrames for each dataset\n",
    "df_ischaemia = pd.DataFrame({\"category\": categories_ischaemia, \"image_path\": image_paths_ischaemia})\n",
    "df_infection = pd.DataFrame({\"category\": categories_infection, \"image_path\": image_paths_infection})\n",
    "\n",
    "# Label encoding for Ischaemia dataset\n",
    "label_encoder_ischaemia = LabelEncoder()\n",
    "df_ischaemia['Class_Label'] = label_encoder_ischaemia.fit_transform(df_ischaemia['category'])\n",
    "print(\"Ischaemia Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_ischaemia['category'].unique(), df_ischaemia['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Label encoding for Infection dataset\n",
    "label_encoder_infection = LabelEncoder()\n",
    "df_infection['Class_Label'] = label_encoder_infection.fit_transform(df_infection['category'])\n",
    "print(\"Infection Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_infection['category'].unique(), df_infection['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Shuffle both DataFrames\n",
    "df_ischaemia = df_ischaemia.sample(frac=1).reset_index(drop=True)\n",
    "df_infection = df_infection.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Helper function to load and process images\n",
    "def load_images(df):\n",
    "    images = []\n",
    "    target_labels = []   \n",
    "    for index, row in df.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        image_array = np.array(image.resize((256, 256)))  # Resize image to fit model input size\n",
    "        images.append(image_array)\n",
    "        target_labels.append(row['Class_Label'])\n",
    "    return np.array(images), np.array(target_labels)\n",
    "\n",
    "# Load images for both datasets\n",
    "images_ischaemia, target_labels_ischaemia = load_images(df_ischaemia)\n",
    "images_infection, target_labels_infection = load_images(df_infection)\n",
    "\n",
    "print(\"Shape of Ischaemia images array:\", images_ischaemia.shape)\n",
    "print(\"Shape of Ischaemia target labels array:\", target_labels_ischaemia.shape)\n",
    "print(\"Shape of Infection images array:\", images_infection.shape)\n",
    "print(\"Shape of Infection target labels array:\", target_labels_infection.shape)\n",
    "\n",
    "# Split the Ischaemia dataset\n",
    "X_train_ischaemia, X_test_ischaemia, y_train_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    images_ischaemia, target_labels_ischaemia, test_size=0.3, random_state=42)\n",
    "X_val_ischaemia, X_test_ischaemia, y_val_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    X_test_ischaemia, y_test_ischaemia, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "# Split the Infection dataset\n",
    "X_train_infection, X_test_infection, y_train_infection, y_test_infection = train_test_split(\n",
    "    images_infection, target_labels_infection, test_size=0.3, random_state=42)\n",
    "X_val_infection, X_test_infection, y_val_infection, y_test_infection = train_test_split(\n",
    "    X_test_infection, y_test_infection, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "print(\"Ischaemia Training set shape:\", X_train_ischaemia.shape, y_train_ischaemia.shape)\n",
    "print(\"Ischaemia Validation set shape:\", X_val_ischaemia.shape, y_val_ischaemia.shape)\n",
    "print(\"Ischaemia Test set shape:\", X_test_ischaemia.shape, y_test_ischaemia.shape)\n",
    "print(\"Infection Training set shape:\", X_train_infection.shape, y_train_infection.shape)\n",
    "print(\"Infection Validation set shape:\", X_val_infection.shape, y_val_infection.shape)\n",
    "print(\"Infection Test set shape:\", X_test_infection.shape, y_test_infection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842988e1-b809-4c6c-bed6-3862c52035bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 15:52:57.313854: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras_tuner import HyperModel, HyperParameters\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Adadelta, SGD, RMSprop\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(filters=int(hp.Int('conv_1_filters', min_value=32, max_value=64, step=16)),\n",
    "                     kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
    "                     activation='relu',\n",
    "                     input_shape=(256, 256, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(filters=int(hp.Int('conv_2_filters', min_value=64, max_value=128, step=16)),\n",
    "                     kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(filters=int(hp.Int('conv_3_filters', min_value=128, max_value=256, step=16)),\n",
    "                     kernel_size=hp.Choice('conv_3_kernel', values=[3, 5]),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    # LSTM Layers\n",
    "    model.add(LSTM(int(hp.Int('lstm_units_1', min_value=80, max_value=120, step=10)), \n",
    "                   dropout=hp.Float('dropout_1', min_value=0.2, max_value=0.3, step=0.1), \n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(int(hp.Int('lstm_units_2', min_value=50, max_value=70, step=10)), \n",
    "                   dropout=hp.Float('dropout_2', min_value=0.2, max_value=0.3, step=0.1), \n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(int(hp.Int('lstm_units_3', min_value=20, max_value=40, step=10)), \n",
    "                   dropout=0.2, \n",
    "                   return_sequences=False))\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(Dense(int(hp.Int('dense_1_units', min_value=128, max_value=512, step=64)), activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(int(hp.Int('dense_2_units', min_value=64, max_value=256, step=64)), activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_4', min_value=0.2, max_value=0.4, step=0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate_params = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "    optimizer_params = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'Adadelta', 'Adagrad'])\n",
    "    \n",
    "    if optimizer_params == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'Adagrad':\n",
    "        optimizer = Adagrad(learning_rate=learning_rate_params)\n",
    "    elif optimizer_params == 'Adadelta':\n",
    "        optimizer = Adadelta(learning_rate=learning_rate_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer choice: {optimizer_choice}\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee85e1b-0bbb-4d3a-99a9-8623efd73523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo\n",
      "Successfully installed py-cpuinfo-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install py-cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b81441-f189-4be1-a27a-ce9adb1c0585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Flags: ['3dnowext', '3dnowprefetch', 'abm', 'adx', 'aes', 'amd_ppin', 'aperfmperf', 'apic', 'arat', 'avic', 'avx', 'avx2', 'bmi1', 'bmi2', 'bpext', 'cat_l3', 'cdp_l3', 'clflush', 'clflushopt', 'clwb', 'clzero', 'cmov', 'cmp_legacy', 'constant_tsc', 'cpb', 'cpuid', 'cqm', 'cqm_llc', 'cqm_mbm_local', 'cqm_mbm_total', 'cqm_occup_llc', 'cr8_legacy', 'cx16', 'cx8', 'dbx', 'de', 'decodeassists', 'extapic', 'extd_apicid', 'f16c', 'flushbyasid', 'fma', 'fpu', 'fsgsbase', 'fxsr', 'fxsr_opt', 'ht', 'hw_pstate', 'ibpb', 'ibrs', 'ibs', 'irperf', 'lahf_lm', 'lbrv', 'lm', 'mba', 'mca', 'mce', 'misalignsse', 'mmx', 'mmxext', 'monitor', 'movbe', 'msr', 'mtrr', 'mwaitx', 'nonstop_tsc', 'nopl', 'npt', 'nrip_save', 'nx', 'osvw', 'osxsave', 'overflow_recov', 'pae', 'pat', 'pausefilter', 'pci_l2i', 'pclmulqdq', 'pdpe1gb', 'perfctr_core', 'perfctr_llc', 'perfctr_nb', 'pfthreshold', 'pge', 'pni', 'popcnt', 'pqe', 'pqm', 'pse', 'pse36', 'rapl', 'rdpid', 'rdpru', 'rdrand', 'rdrnd', 'rdseed', 'rdt_a', 'rdtscp', 'rep_good', 'sep', 'sev', 'sev_es', 'sha', 'sha_ni', 'skinit', 'smap', 'smca', 'smep', 'ssbd', 'sse', 'sse2', 'sse4_1', 'sse4_2', 'sse4a', 'ssse3', 'stibp', 'succor', 'svm', 'svm_lock', 'syscall', 'tce', 'topoext', 'tsc', 'tsc_scale', 'umip', 'v_spec_ctrl', 'v_vmsave_vmload', 'vgif', 'vmcb_clean', 'vme', 'vmmcall', 'wbnoinvd', 'wdt', 'xgetbv1', 'xsave', 'xsavec', 'xsaveerptr', 'xsaveopt']\n",
      "Your CPU supports AVX2 and FMA!\n"
     ]
    }
   ],
   "source": [
    "import cpuinfo\n",
    "\n",
    "info = cpuinfo.get_cpu_info()\n",
    "print(\"CPU Flags:\", info.get('flags', []))\n",
    "\n",
    "# Check for AVX2 and FMA\n",
    "if 'avx2' in info['flags'] and 'fma' in info['flags']:\n",
    "    print(\"Your CPU supports AVX2 and FMA!\")\n",
    "else:\n",
    "    print(\"AVX2 or FMA support is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c639b3e-98b7-4d63-b1c3-8f0f28935023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.16.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (69.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.62.1)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.7)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.8/391.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: flatbuffers, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 24.3.7\n",
      "    Uninstalling flatbuffers-24.3.7:\n",
      "      Successfully uninstalled flatbuffers-24.3.7\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.10.0\n",
      "    Uninstalling h5py-3.10.0:\n",
      "      Successfully uninstalled h5py-3.10.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.0.5\n",
      "    Uninstalling keras-3.0.5:\n",
      "      Successfully uninstalled keras-3.0.5\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.1\n",
      "    Uninstalling tensorflow-2.16.1:\n",
      "      Successfully uninstalled tensorflow-2.16.1\n",
      "Successfully installed flatbuffers-24.3.25 h5py-3.12.1 keras-3.7.0 ml-dtypes-0.4.1 optree-0.13.1 tensorboard-2.18.0 tensorflow-2.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff73af48-0160-4387-810d-894e6f1203b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  kernel_regularizer=None,\n",
      "2024-12-22 15:57:01.807113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22066 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-12-22 15:57:01.810596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22066 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:21:00.0, compute capability: 8.9\n",
      "2024-12-22 15:57:20.979921: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.856274425983429, Training Loss: 0.34269315004348755\n",
      "Validation Accuracy: 0.8540540337562561, Validation Loss: 0.2975222170352936\n",
      "Training Accuracy: 0.8768272995948792, Training Loss: 0.28716257214546204\n",
      "Validation Accuracy: 0.8936936855316162, Validation Loss: 0.23921506106853485\n",
      "Training Accuracy: 0.8218265771865845, Training Loss: 0.40631282329559326\n",
      "Validation Accuracy: 0.8103603720664978, Validation Loss: 0.42306920886039734\n",
      "Training Accuracy: 0.8779852390289307, Training Loss: 0.3022981286048889\n",
      "Validation Accuracy: 0.8738738894462585, Validation Loss: 0.30445581674575806\n",
      "Training Accuracy: 0.8640903234481812, Training Loss: 0.3175891935825348\n",
      "Validation Accuracy: 0.8031531572341919, Validation Loss: 0.389813631772995\n",
      "Training Accuracy: 0.8606165647506714, Training Loss: 0.32313671708106995\n",
      "Validation Accuracy: 0.8869369626045227, Validation Loss: 0.2781726121902466\n",
      "Training Accuracy: 0.875090479850769, Training Loss: 0.30227789282798767\n",
      "Validation Accuracy: 0.8716216087341309, Validation Loss: 0.2916727066040039\n",
      "Training Accuracy: 0.8219713568687439, Training Loss: 0.4093892276287079\n",
      "Validation Accuracy: 0.4981982111930847, Validation Loss: 1.3124440908432007\n",
      "Training Accuracy: 0.8749457001686096, Training Loss: 0.29050013422966003\n",
      "Validation Accuracy: 0.8968468308448792, Validation Loss: 0.24392519891262054\n",
      "Training Accuracy: 0.8645245432853699, Training Loss: 0.315757155418396\n",
      "Validation Accuracy: 0.8585585355758667, Validation Loss: 0.3544289469718933\n",
      "Training Accuracy: 0.838761031627655, Training Loss: 0.3638014495372772\n",
      "Validation Accuracy: 0.8770270347595215, Validation Loss: 0.3554128110408783\n",
      "Training Accuracy: 0.8736431002616882, Training Loss: 0.30351048707962036\n",
      "Validation Accuracy: 0.861261248588562, Validation Loss: 0.42449939250946045\n",
      "Training Accuracy: 0.8899985551834106, Training Loss: 0.2723653316497803\n",
      "Validation Accuracy: 0.885585606098175, Validation Loss: 0.28209853172302246\n",
      "Training Accuracy: 0.8823273777961731, Training Loss: 0.2820908725261688\n",
      "Validation Accuracy: 0.9004504680633545, Validation Loss: 0.25582075119018555\n",
      "Training Accuracy: 0.8626429438591003, Training Loss: 0.3218163251876831\n",
      "Validation Accuracy: 0.842792809009552, Validation Loss: 0.4075043499469757\n",
      "Training Accuracy: 0.8830510973930359, Training Loss: 0.28279346227645874\n",
      "Validation Accuracy: 0.8986486196517944, Validation Loss: 0.24915942549705505\n",
      "Training Accuracy: 0.8745114803314209, Training Loss: 0.2953800559043884\n",
      "Validation Accuracy: 0.8297297358512878, Validation Loss: 0.353226900100708\n",
      "Generation 1, Best Fitness: 0.8297297358512878\n",
      "Training Accuracy: 0.862787663936615, Training Loss: 0.33340775966644287\n",
      "Validation Accuracy: 0.8328828811645508, Validation Loss: 0.3863546848297119\n",
      "Training Accuracy: 0.8677088022232056, Training Loss: 0.3014201819896698\n",
      "Validation Accuracy: 0.8238738775253296, Validation Loss: 0.512713611125946\n",
      "Training Accuracy: 0.8855116367340088, Training Loss: 0.27603834867477417\n",
      "Validation Accuracy: 0.7981982231140137, Validation Loss: 0.455183207988739\n",
      "Training Accuracy: 0.8753799200057983, Training Loss: 0.28298959136009216\n",
      "Validation Accuracy: 0.912162184715271, Validation Loss: 0.2026025652885437\n",
      "Training Accuracy: 0.8413663506507874, Training Loss: 0.36728212237358093\n",
      "Validation Accuracy: 0.8175675868988037, Validation Loss: 0.3895305395126343\n",
      "Training Accuracy: 0.8789983987808228, Training Loss: 0.28673040866851807\n",
      "Validation Accuracy: 0.884684681892395, Validation Loss: 0.3018943667411804\n",
      "Training Accuracy: 0.8679982423782349, Training Loss: 0.316967636346817\n",
      "Validation Accuracy: 0.8774774670600891, Validation Loss: 0.2840847969055176\n",
      "Training Accuracy: 0.878564178943634, Training Loss: 0.29575878381729126\n",
      "Validation Accuracy: 0.8864864706993103, Validation Loss: 0.30454349517822266\n",
      "Training Accuracy: 0.8816037178039551, Training Loss: 0.28085923194885254\n",
      "Validation Accuracy: 0.8896396160125732, Validation Loss: 0.23900571465492249\n",
      "Training Accuracy: 0.8649587631225586, Training Loss: 0.3179835081100464\n",
      "Validation Accuracy: 0.8675675392150879, Validation Loss: 0.31272053718566895\n",
      "Training Accuracy: 0.8703140616416931, Training Loss: 0.3082602322101593\n",
      "Validation Accuracy: 0.8801801800727844, Validation Loss: 0.2646719515323639\n",
      "Training Accuracy: 0.8761036396026611, Training Loss: 0.29382166266441345\n",
      "Validation Accuracy: 0.8504504561424255, Validation Loss: 0.31296005845069885\n",
      "Training Accuracy: 0.8739325404167175, Training Loss: 0.2859671413898468\n",
      "Validation Accuracy: 0.7711711525917053, Validation Loss: 0.5896272659301758\n",
      "Training Accuracy: 0.8651034832000732, Training Loss: 0.3103713393211365\n",
      "Validation Accuracy: 0.722522497177124, Validation Loss: 0.5566998720169067\n",
      "Training Accuracy: 0.8056158423423767, Training Loss: 0.4417690336704254\n",
      "Validation Accuracy: 0.6139639616012573, Validation Loss: 0.764465868473053\n",
      "Training Accuracy: 0.8746562600135803, Training Loss: 0.30276110768318176\n",
      "Validation Accuracy: 0.8216215968132019, Validation Loss: 0.3930949866771698\n",
      "Training Accuracy: 0.8732088804244995, Training Loss: 0.3006902039051056\n",
      "Validation Accuracy: 0.8792792558670044, Validation Loss: 0.31350672245025635\n",
      "Generation 2, Best Fitness: 0.8792792558670044\n",
      "Training Accuracy: 0.8690114617347717, Training Loss: 0.31316205859184265\n",
      "Validation Accuracy: 0.862162172794342, Validation Loss: 0.41807129979133606\n",
      "Training Accuracy: 0.8808799982070923, Training Loss: 0.2920566499233246\n",
      "Validation Accuracy: 0.7788287997245789, Validation Loss: 0.613105297088623\n",
      "Training Accuracy: 0.8737878203392029, Training Loss: 0.2968042492866516\n",
      "Validation Accuracy: 0.8770270347595215, Validation Loss: 0.2783433496952057\n",
      "Training Accuracy: 0.8789983987808228, Training Loss: 0.29140445590019226\n",
      "Validation Accuracy: 0.8675675392150879, Validation Loss: 0.30475959181785583\n",
      "Training Accuracy: 0.8318135738372803, Training Loss: 0.38364675641059875\n",
      "Validation Accuracy: 0.8198198080062866, Validation Loss: 0.4364510178565979\n",
      "Training Accuracy: 0.8779852390289307, Training Loss: 0.2963084578514099\n",
      "Validation Accuracy: 0.869369387626648, Validation Loss: 0.2764436602592468\n",
      "Training Accuracy: 0.8733536005020142, Training Loss: 0.2966018617153168\n",
      "Validation Accuracy: 0.8130630850791931, Validation Loss: 0.42407462000846863\n",
      "Training Accuracy: 0.8787089586257935, Training Loss: 0.2881520986557007\n",
      "Validation Accuracy: 0.8603603839874268, Validation Loss: 0.2952878177165985\n",
      "Training Accuracy: 0.8827615976333618, Training Loss: 0.2837662398815155\n",
      "Validation Accuracy: 0.7990990877151489, Validation Loss: 0.38537082076072693\n",
      "Training Accuracy: 0.8522217273712158, Training Loss: 0.33836686611175537\n",
      "Validation Accuracy: 0.8157657384872437, Validation Loss: 0.4048055112361908\n",
      "Training Accuracy: 0.875090479850769, Training Loss: 0.30725133419036865\n",
      "Validation Accuracy: 0.8900901079177856, Validation Loss: 0.2685563564300537\n",
      "Training Accuracy: 0.8792878985404968, Training Loss: 0.28562572598457336\n",
      "Validation Accuracy: 0.8995495438575745, Validation Loss: 0.2616649866104126\n",
      "Training Accuracy: 0.8745114803314209, Training Loss: 0.2989007532596588\n",
      "Validation Accuracy: 0.8707207441329956, Validation Loss: 0.31987079977989197\n",
      "Training Accuracy: 0.8629323840141296, Training Loss: 0.32303932309150696\n",
      "Validation Accuracy: 0.8554053902626038, Validation Loss: 0.31212353706359863\n",
      "Training Accuracy: 0.8652482032775879, Training Loss: 0.32456591725349426\n",
      "Validation Accuracy: 0.8657657504081726, Validation Loss: 0.3286956250667572\n",
      "Training Accuracy: 0.87132728099823, Training Loss: 0.30064597725868225\n",
      "Validation Accuracy: 0.9031531810760498, Validation Loss: 0.263150691986084\n",
      "Training Accuracy: 0.8791431188583374, Training Loss: 0.28299230337142944\n",
      "Validation Accuracy: 0.8995495438575745, Validation Loss: 0.21168681979179382\n",
      "Generation 3, Best Fitness: 0.8995495438575745\n",
      "Training Accuracy: 0.8739325404167175, Training Loss: 0.2995918393135071\n",
      "Validation Accuracy: 0.8774774670600891, Validation Loss: 0.2918138802051544\n",
      "Training Accuracy: 0.8749457001686096, Training Loss: 0.2963179349899292\n",
      "Validation Accuracy: 0.8698198199272156, Validation Loss: 0.3344030976295471\n",
      "Training Accuracy: 0.8817484378814697, Training Loss: 0.2881522476673126\n",
      "Validation Accuracy: 0.8770270347595215, Validation Loss: 0.3505673110485077\n",
      "Training Accuracy: 0.8640903234481812, Training Loss: 0.319745272397995\n",
      "Validation Accuracy: 0.8864864706993103, Validation Loss: 0.26861223578453064\n",
      "Training Accuracy: 0.8645245432853699, Training Loss: 0.32683658599853516\n",
      "Validation Accuracy: 0.8725225329399109, Validation Loss: 0.32474979758262634\n",
      "Training Accuracy: 0.856274425983429, Training Loss: 0.3337503969669342\n",
      "Validation Accuracy: 0.8828828930854797, Validation Loss: 0.25369733572006226\n",
      "Training Accuracy: 0.8736431002616882, Training Loss: 0.2953427731990814\n",
      "Validation Accuracy: 0.8819819688796997, Validation Loss: 0.3043808937072754\n",
      "Training Accuracy: 0.8704588413238525, Training Loss: 0.3140230178833008\n",
      "Validation Accuracy: 0.8454955220222473, Validation Loss: 0.33546116948127747\n",
      "Training Accuracy: 0.8561297059059143, Training Loss: 0.32574084401130676\n",
      "Validation Accuracy: 0.7905405163764954, Validation Loss: 0.4276329278945923\n",
      "Training Accuracy: 0.8661166429519653, Training Loss: 0.3157951831817627\n",
      "Validation Accuracy: 0.8734233975410461, Validation Loss: 0.2999553084373474\n",
      "Training Accuracy: 0.862787663936615, Training Loss: 0.31856346130371094\n",
      "Validation Accuracy: 0.8923423290252686, Validation Loss: 0.2574557065963745\n",
      "Training Accuracy: 0.8727746605873108, Training Loss: 0.30586904287338257\n",
      "Validation Accuracy: 0.8972973227500916, Validation Loss: 0.25459378957748413\n",
      "Training Accuracy: 0.8643798232078552, Training Loss: 0.3088003396987915\n",
      "Validation Accuracy: 0.8756756782531738, Validation Loss: 0.2765120267868042\n",
      "Training Accuracy: 0.865537703037262, Training Loss: 0.30605533719062805\n",
      "Validation Accuracy: 0.8801801800727844, Validation Loss: 0.2794927954673767\n",
      "Training Accuracy: 0.8652482032775879, Training Loss: 0.3160264194011688\n",
      "Validation Accuracy: 0.8869369626045227, Validation Loss: 0.3008894920349121\n",
      "Training Accuracy: 0.8803010582923889, Training Loss: 0.2905314564704895\n",
      "Validation Accuracy: 0.8545045256614685, Validation Loss: 0.36391934752464294\n",
      "Training Accuracy: 0.8732088804244995, Training Loss: 0.3002363443374634\n",
      "Validation Accuracy: 0.8513513803482056, Validation Loss: 0.35205698013305664\n",
      "Generation 4, Best Fitness: 0.8513513803482056\n",
      "Training Accuracy: 0.8719062209129333, Training Loss: 0.3019293546676636\n",
      "Validation Accuracy: 0.7986486554145813, Validation Loss: 0.423738956451416\n",
      "Training Accuracy: 0.8684324622154236, Training Loss: 0.31311070919036865\n",
      "Validation Accuracy: 0.8698198199272156, Validation Loss: 0.3063649833202362\n",
      "Training Accuracy: 0.8726298809051514, Training Loss: 0.30031028389930725\n",
      "Validation Accuracy: 0.8815315365791321, Validation Loss: 0.2694433629512787\n",
      "Training Accuracy: 0.8632218837738037, Training Loss: 0.3285089433193207\n",
      "Validation Accuracy: 0.8198198080062866, Validation Loss: 0.3700880706310272\n",
      "Training Accuracy: 0.8729193806648254, Training Loss: 0.303201824426651\n",
      "Validation Accuracy: 0.8774774670600891, Validation Loss: 0.30220237374305725\n",
      "Training Accuracy: 0.8707482814788818, Training Loss: 0.3021560609340668\n",
      "Validation Accuracy: 0.861261248588562, Validation Loss: 0.3322836756706238\n",
      "Training Accuracy: 0.8684324622154236, Training Loss: 0.30700504779815674\n",
      "Validation Accuracy: 0.8657657504081726, Validation Loss: 0.3400358259677887\n",
      "Training Accuracy: 0.8752351999282837, Training Loss: 0.2925568222999573\n",
      "Validation Accuracy: 0.8905405402183533, Validation Loss: 0.2691445052623749\n",
      "Training Accuracy: 0.8659719228744507, Training Loss: 0.31772372126579285\n",
      "Validation Accuracy: 0.8761261105537415, Validation Loss: 0.28168967366218567\n",
      "Training Accuracy: 0.8695904016494751, Training Loss: 0.3056453466415405\n",
      "Validation Accuracy: 0.8481981754302979, Validation Loss: 0.3668748438358307\n",
      "Training Accuracy: 0.8578665256500244, Training Loss: 0.33575549721717834\n",
      "Validation Accuracy: 0.8909909725189209, Validation Loss: 0.26354748010635376\n",
      "Training Accuracy: 0.8729193806648254, Training Loss: 0.2957431375980377\n",
      "Validation Accuracy: 0.7972972989082336, Validation Loss: 0.4627324342727661\n",
      "Training Accuracy: 0.8732088804244995, Training Loss: 0.3015376627445221\n",
      "Validation Accuracy: 0.8734233975410461, Validation Loss: 0.3872830271720886\n",
      "Training Accuracy: 0.8639456033706665, Training Loss: 0.32044461369514465\n",
      "Validation Accuracy: 0.75, Validation Loss: 0.5497032999992371\n",
      "Training Accuracy: 0.8313793540000916, Training Loss: 0.40295395255088806\n",
      "Validation Accuracy: 0.8297297358512878, Validation Loss: 0.39945271611213684\n",
      "Training Accuracy: 0.8668403625488281, Training Loss: 0.31473541259765625\n",
      "Validation Accuracy: 0.8932432532310486, Validation Loss: 0.2706008851528168\n",
      "Training Accuracy: 0.8646692633628845, Training Loss: 0.31394124031066895\n",
      "Validation Accuracy: 0.8545045256614685, Validation Loss: 0.4722551703453064\n",
      "Generation 5, Best Fitness: 0.8545045256614685\n",
      "Best Hyperparameters after EHO Optimization:\n",
      "{'conv_1_filters': 49.99793567417412, 'conv_2_filters': 109.6323018163025, 'conv_3_filters': 176.3041985125654, 'lstm_units_1': 101.3204429818439, 'lstm_units_2': 52.85361832069488, 'lstm_units_3': 30.32950520090426, 'dropout_1': 0.24303943056489039, 'dropout_2': 0.21149913222351552, 'dropout_3': 0.38054428646694954, 'dropout_4': 0.3018171226563811, 'dense_1_units': 393.40895720087315, 'dense_2_units': 202.5062500031708, 'learning_rate': 0.00256312505615965}\n",
      "Epoch 1/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - accuracy: 0.6872 - loss: 0.8219 - val_accuracy: 0.6635 - val_loss: 0.5509\n",
      "Epoch 2/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8249 - loss: 0.4024 - val_accuracy: 0.8149 - val_loss: 0.4103\n",
      "Epoch 3/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.8356 - loss: 0.3795 - val_accuracy: 0.8613 - val_loss: 0.3389\n",
      "Epoch 4/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8493 - loss: 0.3515 - val_accuracy: 0.6977 - val_loss: 0.5641\n",
      "Epoch 5/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.8564 - loss: 0.3420 - val_accuracy: 0.5306 - val_loss: 1.5251\n",
      "Epoch 6/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.8673 - loss: 0.3229 - val_accuracy: 0.8523 - val_loss: 0.3768\n",
      "Epoch 7/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.8819 - loss: 0.2954 - val_accuracy: 0.8113 - val_loss: 0.3673\n",
      "Epoch 8/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.8701 - loss: 0.3091 - val_accuracy: 0.8239 - val_loss: 0.3859\n",
      "Epoch 9/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.8654 - loss: 0.3169 - val_accuracy: 0.8955 - val_loss: 0.2499\n",
      "Epoch 10/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8717 - loss: 0.2972 - val_accuracy: 0.8892 - val_loss: 0.2849\n",
      "Best Model Training Accuracy: 0.8719062209129333, Training Loss: 0.29354622960090637\n",
      "Best Model Validation Accuracy: 0.8891891837120056, Validation Loss: 0.2849056124687195\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9078 - loss: 0.2760\n",
      "Best Model Test Accuracy: 0.9041835069656372, Test Loss: 0.26537010073661804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# EHO Parameters\n",
    "num_clans = 1  # Since population size is 5, let's use 1 clan with 5 elephants for simplicity\n",
    "num_elephants_per_clan = 5\n",
    "max_generations = 5\n",
    "epochs = 10  # Number of training epochs\n",
    "\n",
    "# Boundaries for the hyperparameters\n",
    "bounds = {\n",
    "    'conv_1_filters': [32, 64],\n",
    "    'conv_2_filters': [64, 128],\n",
    "    'conv_3_filters': [128, 256],\n",
    "    'lstm_units_1': [80, 120],\n",
    "    'lstm_units_2': [50, 70],\n",
    "    'lstm_units_3': [20, 40],\n",
    "    'dropout_1': [0.2, 0.3],\n",
    "    'dropout_2': [0.2, 0.3],\n",
    "    'dropout_3': [0.3, 0.5],\n",
    "    'dropout_4': [0.2, 0.4],\n",
    "    'dense_1_units': [128, 512],\n",
    "    'dense_2_units': [64, 256],\n",
    "    'learning_rate': [1e-4, 1e-2]\n",
    "}\n",
    "\n",
    "# Clan structure\n",
    "def initialize_population():\n",
    "    population = []\n",
    "    for _ in range(num_clans):\n",
    "        clan = []\n",
    "        for _ in range(num_elephants_per_clan):\n",
    "            elephant = {param: random.uniform(bounds[param][0], bounds[param][1]) for param in bounds}\n",
    "            clan.append(elephant)\n",
    "        population.append(clan)\n",
    "    return population\n",
    "\n",
    "# Fitness evaluation (e.g., model accuracy)\n",
    "def evaluate_fitness(elephant):\n",
    "    # Define hyperparameters using the current elephant's values\n",
    "    hp = HyperParameters()\n",
    "    for param, value in elephant.items():\n",
    "        hp.Fixed(param, value)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = build_model(hp)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_ischaemia, y_train_ischaemia, \n",
    "        validation_data=(X_val_ischaemia, y_val_ischaemia), \n",
    "        epochs=epochs,  # Set epochs to 10\n",
    "        verbose=0  # Suppress detailed logs\n",
    "    )\n",
    "    \n",
    "    # Retrieve the final epoch's metrics\n",
    "    training_accuracy = history.history['accuracy'][-1]\n",
    "    training_loss = history.history['loss'][-1]\n",
    "    validation_accuracy = history.history['val_accuracy'][-1]\n",
    "    validation_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    # Print the training and validation metrics\n",
    "    print(f\"Training Accuracy: {training_accuracy}, Training Loss: {training_loss}\")\n",
    "    print(f\"Validation Accuracy: {validation_accuracy}, Validation Loss: {validation_loss}\")\n",
    "    \n",
    "    # Return validation accuracy as fitness score\n",
    "    return validation_accuracy\n",
    "\n",
    "# Clan updating operator\n",
    "def clan_update(clan, matriarch):\n",
    "    for elephant in clan:\n",
    "        for param in bounds:\n",
    "            beta = np.random.uniform(0, 1)\n",
    "            elephant[param] = beta * matriarch[param] + (1 - beta) * elephant[param]\n",
    "    return clan\n",
    "\n",
    "# Separating operator\n",
    "def separating_operator(clan):\n",
    "    worst_elephant = sorted(clan, key=evaluate_fitness)[0]\n",
    "    for param in worst_elephant:\n",
    "        worst_elephant[param] = random.uniform(bounds[param][0], bounds[param][1])\n",
    "    return clan\n",
    "\n",
    "# EHO main loop\n",
    "def eho_optimization():\n",
    "    population = initialize_population()\n",
    "    \n",
    "    best_solution = None\n",
    "    best_fitness = -float('inf')\n",
    "    \n",
    "    for generation in range(max_generations):\n",
    "        for clan in population:\n",
    "            # Find the matriarch (best elephant)\n",
    "            matriarch = max(clan, key=evaluate_fitness)\n",
    "            # Update clan members\n",
    "            clan = clan_update(clan, matriarch)\n",
    "            # Perform separating operation on worst elephant\n",
    "            clan = separating_operator(clan)\n",
    "        \n",
    "        # Find the best solution in the current generation\n",
    "        current_best = max([max(clan, key=evaluate_fitness) for clan in population], key=evaluate_fitness)\n",
    "        current_best_fitness = evaluate_fitness(current_best)\n",
    "        \n",
    "        if current_best_fitness > best_fitness:\n",
    "            best_fitness = current_best_fitness\n",
    "            best_solution = current_best\n",
    "        \n",
    "        print(f\"Generation {generation+1}, Best Fitness: {current_best_fitness}\")\n",
    "    \n",
    "    return best_solution\n",
    "\n",
    "# Run the EHO optimization\n",
    "best_hyperparameters = eho_optimization()\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters after EHO Optimization:\")\n",
    "print(best_hyperparameters)\n",
    "\n",
    "# Evaluate the best model with test accuracy\n",
    "def evaluate_best_hyperparameters(best_hyperparameters):\n",
    "    # Define the best hyperparameters using the final best solution\n",
    "    hp = HyperParameters()\n",
    "    for param, value in best_hyperparameters.items():\n",
    "        hp.Fixed(param, value)\n",
    "    \n",
    "    # Build and compile the model with best hyperparameters\n",
    "    model = build_model(hp)\n",
    "    \n",
    "    # Train the model with best hyperparameters\n",
    "    history = model.fit(\n",
    "        X_train_ischaemia, y_train_ischaemia, \n",
    "        validation_data=(X_val_ischaemia, y_val_ischaemia), \n",
    "        epochs=epochs,  # Keep it consistent with 10 epochs\n",
    "        verbose=1  # Show detailed logs now\n",
    "    )\n",
    "    \n",
    "    # Print the final metrics for the best hyperparameters\n",
    "    training_accuracy = history.history['accuracy'][-1]\n",
    "    training_loss = history.history['loss'][-1]\n",
    "    validation_accuracy = history.history['val_accuracy'][-1]\n",
    "    validation_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    print(f\"Best Model Training Accuracy: {training_accuracy}, Training Loss: {training_loss}\")\n",
    "    print(f\"Best Model Validation Accuracy: {validation_accuracy}, Validation Loss: {validation_loss}\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_ischaemia, y_test_ischaemia, verbose=1)\n",
    "    print(f\"Best Model Test Accuracy: {test_accuracy}, Test Loss: {test_loss}\")\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluate_best_hyperparameters(best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b562673-ffe2-4b78-b1d7-5b36d22b4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - accuracy: 0.6776 - loss: 0.8280 - val_accuracy: 0.7874 - val_loss: 0.4809\n",
      "Epoch 2/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8174 - loss: 0.4129 - val_accuracy: 0.8532 - val_loss: 0.3696\n",
      "Epoch 3/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.8331 - loss: 0.3883 - val_accuracy: 0.8315 - val_loss: 0.4367\n",
      "Epoch 4/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.8375 - loss: 0.3774 - val_accuracy: 0.8581 - val_loss: 0.2837\n",
      "Epoch 5/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.8468 - loss: 0.3472 - val_accuracy: 0.8716 - val_loss: 0.2892\n",
      "Epoch 6/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 174ms/step - accuracy: 0.8467 - loss: 0.3496 - val_accuracy: 0.8302 - val_loss: 0.3732\n",
      "Epoch 7/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.8714 - loss: 0.3007 - val_accuracy: 0.8473 - val_loss: 0.3274\n",
      "Epoch 8/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.8750 - loss: 0.2800 - val_accuracy: 0.8581 - val_loss: 0.3084\n",
      "Epoch 9/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 176ms/step - accuracy: 0.8717 - loss: 0.2942 - val_accuracy: 0.8716 - val_loss: 0.2899\n",
      "Epoch 10/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.8767 - loss: 0.2869 - val_accuracy: 0.8041 - val_loss: 0.4680\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8101 - loss: 0.4803\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "Training Accuracy: 0.8714720010757446\n",
      "Training Loss: 0.3007606565952301\n",
      "Training F1 Score: 0.7885796582315139\n",
      "Training Precision: 0.8325313978374304\n",
      "Training Recall: 0.7958745789950591\n",
      "Training Time (s): 397.1320767402649\n",
      "Validation Accuracy: 0.8040540814399719\n",
      "Validation Loss: 0.4680430293083191\n",
      "Validation F1 Score: 0.7976479216323109\n",
      "Validation Precision: 0.8442771885248324\n",
      "Validation Recall: 0.8026641634585181\n",
      "Validation Time (s): 4.552062273025513\n",
      "Test Accuracy: 0.8016194105148315\n",
      "Test F1 Score: 0.788883569754844\n",
      "Test Precision: 0.8448833855137721\n",
      "Test Recall: 0.7873851294903926\n",
      "Testing Time (s): 3.135561466217041\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to calculate metrics for the best hyperparameters\n",
    "def evaluate_best_hyperparameters(best_hyperparameters):\n",
    "    # Define the best hyperparameters using the final best solution\n",
    "    hp = HyperParameters()\n",
    "    for param, value in best_hyperparameters.items():\n",
    "        hp.Fixed(param, value)\n",
    "    \n",
    "    # Build and compile the model with best hyperparameters\n",
    "    model = build_model(hp)\n",
    "    \n",
    "    # Start training\n",
    "    start_training_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train_ischaemia, y_train_ischaemia,\n",
    "        validation_data=(X_val_ischaemia, y_val_ischaemia),\n",
    "        epochs=10,  # Keep it consistent with the notebook's configuration\n",
    "        verbose=1  # Show detailed logs\n",
    "    )\n",
    "    end_training_time = time.time()\n",
    "    training_time = end_training_time - start_training_time\n",
    "    \n",
    "    # Collect training metrics\n",
    "    train_accuracy = history.history['accuracy'][-1]\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    y_train_pred = np.argmax(model.predict(X_train_ischaemia), axis=1)\n",
    "    train_f1 = f1_score(y_train_ischaemia, y_train_pred, average='macro')\n",
    "    train_precision = precision_score(y_train_ischaemia, y_train_pred, average='macro')\n",
    "    train_recall = recall_score(y_train_ischaemia, y_train_pred, average='macro')\n",
    "    \n",
    "    # Validation metrics\n",
    "    start_validation_time = time.time()\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_ischaemia, y_val_ischaemia)\n",
    "    end_validation_time = time.time()\n",
    "    validation_time = end_validation_time - start_validation_time\n",
    "    \n",
    "    y_val_pred = np.argmax(model.predict(X_val_ischaemia), axis=1)\n",
    "    val_f1 = f1_score(y_val_ischaemia, y_val_pred, average='macro')\n",
    "    val_precision = precision_score(y_val_ischaemia, y_val_pred, average='macro')\n",
    "    val_recall = recall_score(y_val_ischaemia, y_val_pred, average='macro')\n",
    "    \n",
    "    # Testing metrics\n",
    "    start_testing_time = time.time()\n",
    "    test_accuracy = model.evaluate(X_test_ischaemia, y_test_ischaemia, verbose=0)[1]\n",
    "    y_test_pred = model.predict(X_test_ischaemia)\n",
    "    y_test_classes = np.argmax(y_test_pred, axis=1)\n",
    "    end_testing_time = time.time()\n",
    "    testing_time = end_testing_time - start_testing_time\n",
    "    \n",
    "    test_f1 = f1_score(y_test_ischaemia, y_test_classes, average='macro')\n",
    "    test_precision = precision_score(y_test_ischaemia, y_test_classes, average='macro')\n",
    "    test_recall = recall_score(y_test_ischaemia, y_test_classes, average='macro')\n",
    "    \n",
    "    # Collect metrics in a dictionary\n",
    "    metrics = {\n",
    "        'Training Accuracy': train_accuracy,\n",
    "        'Training Loss': train_loss,\n",
    "        'Training F1 Score': train_f1,\n",
    "        'Training Precision': train_precision,\n",
    "        'Training Recall': train_recall,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Validation Accuracy': val_accuracy,\n",
    "        'Validation Loss': val_loss,\n",
    "        'Validation F1 Score': val_f1,\n",
    "        'Validation Precision': val_precision,\n",
    "        'Validation Recall': val_recall,\n",
    "        'Validation Time (s)': validation_time,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test F1 Score': test_f1,\n",
    "        'Test Precision': test_precision,\n",
    "        'Test Recall': test_recall,\n",
    "        'Testing Time (s)': testing_time\n",
    "    }\n",
    "    \n",
    "    # Print the metrics\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "# Evaluate the best hyperparameters after EHO optimization\n",
    "evaluate_best_hyperparameters(best_hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd3bacc7-6722-4805-8e6b-8dc23b6f534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  kernel_regularizer=None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 190ms/step - accuracy: 0.7036 - loss: 0.8219 - val_accuracy: 0.8023 - val_loss: 0.4913\n",
      "Epoch 2/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8202 - loss: 0.4003 - val_accuracy: 0.8081 - val_loss: 0.4043\n",
      "Epoch 3/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.8401 - loss: 0.3689 - val_accuracy: 0.8671 - val_loss: 0.3397\n",
      "Epoch 4/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.8346 - loss: 0.3665 - val_accuracy: 0.8514 - val_loss: 0.3230\n",
      "Epoch 5/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.8574 - loss: 0.3237 - val_accuracy: 0.7977 - val_loss: 0.6633\n",
      "Epoch 6/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.8599 - loss: 0.3354 - val_accuracy: 0.8631 - val_loss: 0.3067\n",
      "Epoch 7/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.8696 - loss: 0.3066 - val_accuracy: 0.8054 - val_loss: 0.4038\n",
      "Epoch 8/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.8686 - loss: 0.3115 - val_accuracy: 0.8968 - val_loss: 0.2748\n",
      "Epoch 9/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8683 - loss: 0.2896 - val_accuracy: 0.8455 - val_loss: 0.4247\n",
      "Epoch 10/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.8726 - loss: 0.2918 - val_accuracy: 0.8968 - val_loss: 0.2308\n",
      "Epoch 11/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.8874 - loss: 0.2669 - val_accuracy: 0.8928 - val_loss: 0.2555\n",
      "Epoch 12/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8817 - loss: 0.2667 - val_accuracy: 0.8437 - val_loss: 0.3876\n",
      "Epoch 13/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.8781 - loss: 0.2872 - val_accuracy: 0.8937 - val_loss: 0.2537\n",
      "Epoch 14/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.8897 - loss: 0.2621 - val_accuracy: 0.9009 - val_loss: 0.2363\n",
      "Epoch 15/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.8947 - loss: 0.2527 - val_accuracy: 0.8536 - val_loss: 0.3496\n",
      "Epoch 16/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 176ms/step - accuracy: 0.8949 - loss: 0.2587 - val_accuracy: 0.8977 - val_loss: 0.2446\n",
      "Epoch 17/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.8860 - loss: 0.2641 - val_accuracy: 0.8689 - val_loss: 0.3029\n",
      "Epoch 18/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.8843 - loss: 0.2639 - val_accuracy: 0.9095 - val_loss: 0.2199\n",
      "Epoch 19/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 177ms/step - accuracy: 0.8896 - loss: 0.2602 - val_accuracy: 0.8874 - val_loss: 0.2537\n",
      "Epoch 20/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.8953 - loss: 0.2445 - val_accuracy: 0.9216 - val_loss: 0.1929\n",
      "Epoch 21/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.9011 - loss: 0.2457 - val_accuracy: 0.8428 - val_loss: 0.4761\n",
      "Epoch 22/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.9024 - loss: 0.2366 - val_accuracy: 0.8910 - val_loss: 0.2990\n",
      "Epoch 23/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8901 - loss: 0.2495 - val_accuracy: 0.9090 - val_loss: 0.2102\n",
      "Epoch 24/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.9055 - loss: 0.2317 - val_accuracy: 0.8887 - val_loss: 0.2594\n",
      "Epoch 25/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.8942 - loss: 0.2366 - val_accuracy: 0.8914 - val_loss: 0.2915\n",
      "Epoch 26/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.8993 - loss: 0.2549 - val_accuracy: 0.9167 - val_loss: 0.1978\n",
      "Epoch 27/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.9001 - loss: 0.2414 - val_accuracy: 0.8869 - val_loss: 0.2965\n",
      "Epoch 28/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9031 - loss: 0.2217 - val_accuracy: 0.9081 - val_loss: 0.2106\n",
      "Epoch 29/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9043 - loss: 0.2182 - val_accuracy: 0.8419 - val_loss: 0.3953\n",
      "Epoch 30/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.9069 - loss: 0.2237 - val_accuracy: 0.9212 - val_loss: 0.2180\n",
      "Epoch 31/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9062 - loss: 0.2290 - val_accuracy: 0.8919 - val_loss: 0.2637\n",
      "Epoch 32/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - accuracy: 0.9017 - loss: 0.2384 - val_accuracy: 0.9081 - val_loss: 0.2562\n",
      "Epoch 33/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 176ms/step - accuracy: 0.9095 - loss: 0.2187 - val_accuracy: 0.9149 - val_loss: 0.2433\n",
      "Epoch 34/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.9081 - loss: 0.2311 - val_accuracy: 0.9086 - val_loss: 0.2183\n",
      "Epoch 35/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.9104 - loss: 0.2242 - val_accuracy: 0.8946 - val_loss: 0.2814\n",
      "Epoch 36/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.9148 - loss: 0.2103 - val_accuracy: 0.8928 - val_loss: 0.2709\n",
      "Epoch 37/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.9179 - loss: 0.2016 - val_accuracy: 0.9234 - val_loss: 0.1848\n",
      "Epoch 38/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.9170 - loss: 0.1975 - val_accuracy: 0.8964 - val_loss: 0.2748\n",
      "Epoch 39/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9102 - loss: 0.2235 - val_accuracy: 0.9162 - val_loss: 0.2191\n",
      "Epoch 40/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - accuracy: 0.9123 - loss: 0.2310 - val_accuracy: 0.9189 - val_loss: 0.1834\n",
      "Epoch 41/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9194 - loss: 0.1991 - val_accuracy: 0.8856 - val_loss: 0.2852\n",
      "Epoch 42/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9140 - loss: 0.2059 - val_accuracy: 0.8009 - val_loss: 0.4708\n",
      "Epoch 43/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 178ms/step - accuracy: 0.9101 - loss: 0.2066 - val_accuracy: 0.9284 - val_loss: 0.1887\n",
      "Epoch 44/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 176ms/step - accuracy: 0.9304 - loss: 0.1702 - val_accuracy: 0.8734 - val_loss: 0.3094\n",
      "Epoch 45/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9103 - loss: 0.2123 - val_accuracy: 0.9194 - val_loss: 0.1874\n",
      "Epoch 46/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9109 - loss: 0.2170 - val_accuracy: 0.8599 - val_loss: 0.3280\n",
      "Epoch 47/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.9133 - loss: 0.2138 - val_accuracy: 0.9032 - val_loss: 0.2374\n",
      "Epoch 48/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9172 - loss: 0.2052 - val_accuracy: 0.9162 - val_loss: 0.2654\n",
      "Epoch 49/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 180ms/step - accuracy: 0.9233 - loss: 0.1929 - val_accuracy: 0.9216 - val_loss: 0.1877\n",
      "Epoch 50/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 179ms/step - accuracy: 0.9099 - loss: 0.2271 - val_accuracy: 0.5536 - val_loss: 1.0305\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.5602 - loss: 1.0361\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "Training Accuracy: 0.9106962084770203\n",
      "Training Loss: 0.22172869741916656\n",
      "Training F1 Score: 0.45420149087521217\n",
      "Training Precision: 0.7571921704099525\n",
      "Training Recall: 0.5578275239931332\n",
      "Training Time (s): 1947.8449959754944\n",
      "Validation Accuracy: 0.5536035895347595\n",
      "Validation Loss: 1.0305231809616089\n",
      "Validation F1 Score: 0.44907371351742953\n",
      "Validation Precision: 0.7519890957484867\n",
      "Validation Recall: 0.5571720078992288\n",
      "Validation Time (s): 4.530449628829956\n",
      "Test Accuracy: 0.519568145275116\n",
      "Test F1 Score: 0.426131221719457\n",
      "Test Precision: 0.7449856733524356\n",
      "Test Recall: 0.5538847117794486\n",
      "Testing Time (s): 3.104048013687134\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to calculate metrics for the best hyperparameters\n",
    "def evaluate_best_hyperparameters(best_hyperparameters):\n",
    "    # Define the best hyperparameters using the final best solution\n",
    "    hp = HyperParameters()\n",
    "    for param, value in best_hyperparameters.items():\n",
    "        hp.Fixed(param, value)\n",
    "    \n",
    "    # Build and compile the model with best hyperparameters\n",
    "    model = build_model(hp)\n",
    "    \n",
    "    # Start training\n",
    "    start_training_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train_ischaemia, y_train_ischaemia,\n",
    "        validation_data=(X_val_ischaemia, y_val_ischaemia),\n",
    "        epochs=50,\n",
    "        verbose=1 \n",
    "    )\n",
    "    end_training_time = time.time()\n",
    "    training_time = end_training_time - start_training_time\n",
    "    \n",
    "    # Collect training metrics\n",
    "    train_accuracy = history.history['accuracy'][-1]\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    y_train_pred = np.argmax(model.predict(X_train_ischaemia), axis=1)\n",
    "    train_f1 = f1_score(y_train_ischaemia, y_train_pred, average='macro')\n",
    "    train_precision = precision_score(y_train_ischaemia, y_train_pred, average='macro')\n",
    "    train_recall = recall_score(y_train_ischaemia, y_train_pred, average='macro')\n",
    "    \n",
    "    # Validation metrics\n",
    "    start_validation_time = time.time()\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_ischaemia, y_val_ischaemia)\n",
    "    end_validation_time = time.time()\n",
    "    validation_time = end_validation_time - start_validation_time\n",
    "    \n",
    "    y_val_pred = np.argmax(model.predict(X_val_ischaemia), axis=1)\n",
    "    val_f1 = f1_score(y_val_ischaemia, y_val_pred, average='macro')\n",
    "    val_precision = precision_score(y_val_ischaemia, y_val_pred, average='macro')\n",
    "    val_recall = recall_score(y_val_ischaemia, y_val_pred, average='macro')\n",
    "    \n",
    "    # Testing metrics\n",
    "    start_testing_time = time.time()\n",
    "    test_accuracy = model.evaluate(X_test_ischaemia, y_test_ischaemia, verbose=0)[1]\n",
    "    y_test_pred = model.predict(X_test_ischaemia)\n",
    "    y_test_classes = np.argmax(y_test_pred, axis=1)\n",
    "    end_testing_time = time.time()\n",
    "    testing_time = end_testing_time - start_testing_time\n",
    "    \n",
    "    test_f1 = f1_score(y_test_ischaemia, y_test_classes, average='macro')\n",
    "    test_precision = precision_score(y_test_ischaemia, y_test_classes, average='macro')\n",
    "    test_recall = recall_score(y_test_ischaemia, y_test_classes, average='macro')\n",
    "    \n",
    "    # Collect metrics in a dictionary\n",
    "    metrics = {\n",
    "        'Training Accuracy': train_accuracy,\n",
    "        'Training Loss': train_loss,\n",
    "        'Training F1 Score': train_f1,\n",
    "        'Training Precision': train_precision,\n",
    "        'Training Recall': train_recall,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Validation Accuracy': val_accuracy,\n",
    "        'Validation Loss': val_loss,\n",
    "        'Validation F1 Score': val_f1,\n",
    "        'Validation Precision': val_precision,\n",
    "        'Validation Recall': val_recall,\n",
    "        'Validation Time (s)': validation_time,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test F1 Score': test_f1,\n",
    "        'Test Precision': test_precision,\n",
    "        'Test Recall': test_recall,\n",
    "        'Testing Time (s)': testing_time\n",
    "    }\n",
    "    \n",
    "    # Print the metrics\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "# Evaluate the best hyperparameters after EHO optimization\n",
    "evaluate_best_hyperparameters(best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095467a-9f6c-40ba-95e1-2c9cb5d48133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
