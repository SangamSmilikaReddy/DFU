{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46002e6-eaad-47a9-8292-2188203ece6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 14:18:24.758642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 14:18:24.776402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 14:18:24.781854: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 14:18:24.795637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed successfully to 'DFU_dataset'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile as zf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "# Path to the repaired ZIP file\n",
    "zip_file_path = \"PartB_DFU_dataset - Copy.zip\"\n",
    "extract_path = \"DFU_dataset\"\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    try:\n",
    "        with zf.ZipFile(zip_file_path, 'r') as files:\n",
    "            files.extractall(extract_path)\n",
    "        print(f\"Extraction completed successfully to '{extract_path}'\")\n",
    "    except zf.BadZipFile:\n",
    "        print(\"Error: The ZIP file is corrupted.\")\n",
    "    except OSError as e:\n",
    "        print(f\"OS error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"Error: The file '{zip_file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84251448-be88-4fd5-bc82-10122731381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ischaemia Class Mapping:\n",
      "non-ischemia: 1\n",
      "ischemia: 0\n",
      "Infection Class Mapping:\n",
      "non-infection: 1\n",
      "infection: 0\n",
      "Shape of Ischaemia images array: (9870, 224, 224, 3)\n",
      "Shape of Ischaemia target labels array: (9870,)\n",
      "Shape of Infection images array: (5890, 224, 224, 3)\n",
      "Shape of Infection target labels array: (5890,)\n",
      "Ischaemia Training set shape: (6909, 224, 224, 3) (6909,)\n",
      "Ischaemia Validation set shape: (2220, 224, 224, 3) (2220,)\n",
      "Ischaemia Test set shape: (741, 224, 224, 3) (741,)\n",
      "Infection Training set shape: (4123, 224, 224, 3) (4123,)\n",
      "Infection Validation set shape: (1325, 224, 224, 3) (1325,)\n",
      "Infection Test set shape: (442, 224, 224, 3) (442,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Define the root directory where your image folders are located\n",
    "root_directory = \"DFU_dataset/PartB_DFU_dataset - Copy\"\n",
    "\n",
    "# Initialize lists to store image paths and corresponding class labels for both datasets\n",
    "image_paths_ischaemia = []\n",
    "categories_ischaemia = []\n",
    "image_paths_infection = []\n",
    "categories_infection = []\n",
    "\n",
    "# Iterate over each class and its subdirectories\n",
    "for class_name in [\"Infection\", \"Ischaemia\"]:\n",
    "    for augmentation_type in [\"Aug-Negative\", \"Aug-Positive\"]:\n",
    "        folder_path = os.path.join(root_directory, class_name, augmentation_type)\n",
    "        category = f\"{class_name.lower()}{'pov' if 'Positive' in augmentation_type else 'neg'}\"\n",
    "        \n",
    "        # Iterate over image files in the current directory\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".jpg\"):  # Assuming images are jpg format\n",
    "                image_path = os.path.join(folder_path, file_name)\n",
    "                if class_name == \"Ischaemia\":\n",
    "                    image_paths_ischaemia.append(image_path)\n",
    "                    categories_ischaemia.append(\"ischemia\" if \"Positive\" in augmentation_type else \"non-ischemia\")\n",
    "                elif class_name == \"Infection\":\n",
    "                    image_paths_infection.append(image_path)\n",
    "                    categories_infection.append(\"infection\" if \"Positive\" in augmentation_type else \"non-infection\")\n",
    "\n",
    "# Create DataFrames for each dataset\n",
    "df_ischaemia = pd.DataFrame({\"category\": categories_ischaemia, \"image_path\": image_paths_ischaemia})\n",
    "df_infection = pd.DataFrame({\"category\": categories_infection, \"image_path\": image_paths_infection})\n",
    "\n",
    "# Label encoding for Ischaemia dataset\n",
    "label_encoder_ischaemia = LabelEncoder()\n",
    "df_ischaemia['Class_Label'] = label_encoder_ischaemia.fit_transform(df_ischaemia['category'])\n",
    "print(\"Ischaemia Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_ischaemia['category'].unique(), df_ischaemia['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Label encoding for Infection dataset\n",
    "label_encoder_infection = LabelEncoder()\n",
    "df_infection['Class_Label'] = label_encoder_infection.fit_transform(df_infection['category'])\n",
    "print(\"Infection Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_infection['category'].unique(), df_infection['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Shuffle both DataFrames\n",
    "df_ischaemia = df_ischaemia.sample(frac=1).reset_index(drop=True)\n",
    "df_infection = df_infection.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Helper function to load and process images\n",
    "def load_images(df):\n",
    "    images = []\n",
    "    target_labels = []   \n",
    "    for index, row in df.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        image_array = np.array(image.resize((224, 224)))  # Resize image to fit MobileNet input size\n",
    "        images.append(image_array)\n",
    "        target_labels.append(row['Class_Label'])\n",
    "    return np.array(images), np.array(target_labels)\n",
    "\n",
    "# Load images for both datasets\n",
    "images_ischaemia, target_labels_ischaemia = load_images(df_ischaemia)\n",
    "images_infection, target_labels_infection = load_images(df_infection)\n",
    "\n",
    "print(\"Shape of Ischaemia images array:\", images_ischaemia.shape)\n",
    "print(\"Shape of Ischaemia target labels array:\", target_labels_ischaemia.shape)\n",
    "print(\"Shape of Infection images array:\", images_infection.shape)\n",
    "print(\"Shape of Infection target labels array:\", target_labels_infection.shape)\n",
    "\n",
    "# Split the Ischaemia dataset\n",
    "X_train_ischaemia, X_test_ischaemia, y_train_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    images_ischaemia, target_labels_ischaemia, test_size=0.3, random_state=42)\n",
    "X_val_ischaemia, X_test_ischaemia, y_val_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    X_test_ischaemia, y_test_ischaemia, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "# Split the Infection dataset\n",
    "X_train_infection, X_test_infection, y_train_infection, y_test_infection = train_test_split(\n",
    "    images_infection, target_labels_infection, test_size=0.3, random_state=42)\n",
    "X_val_infection, X_test_infection, y_val_infection, y_test_infection = train_test_split(\n",
    "    X_test_infection, y_test_infection, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "print(\"Ischaemia Training set shape:\", X_train_ischaemia.shape, y_train_ischaemia.shape)\n",
    "print(\"Ischaemia Validation set shape:\", X_val_ischaemia.shape, y_val_ischaemia.shape)\n",
    "print(\"Ischaemia Test set shape:\", X_test_ischaemia.shape, y_test_ischaemia.shape)\n",
    "print(\"Infection Training set shape:\", X_train_infection.shape, y_train_infection.shape)\n",
    "print(\"Infection Validation set shape:\", X_val_infection.shape, y_val_infection.shape)\n",
    "print(\"Infection Test set shape:\", X_test_infection.shape, y_test_infection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1543374a-8801-4ec1-a6a0-405ce790e34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 03s]\n",
      "val_accuracy: 0.8090090155601501\n",
      "\n",
      "Best val_accuracy So Far: 0.8135135173797607\n",
      "Total elapsed time: 00h 05m 20s\n",
      "   trial_number  accuracy      loss  val_accuracy  val_loss  training_time  \\\n",
      "0             1  0.816471  0.410784      0.804955  0.429463      53.363787   \n",
      "1             2  0.814734  0.413955      0.794595  0.430587      53.402954   \n",
      "2             3  0.798668  0.435290      0.776577  0.488355      53.380224   \n",
      "3             4  0.816905  0.410006      0.813514  0.409536      53.417816   \n",
      "4             5  0.822261  0.405351      0.809009  0.411479      53.130682   \n",
      "\n",
      "  verbose  epochs  steps  \n",
      "0    auto      10    216  \n",
      "1    auto      10    216  \n",
      "2    auto      10    216  \n",
      "3    auto      10    216  \n",
      "4    auto      10    216  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from kerastuner import HyperModel, HyperParameters\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import Dropout, TimeDistributed, Flatten, LSTM, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    base_model = MobileNet(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        TimeDistributed(Flatten()),\n",
    "        LSTM(hp.Int('lstm_units_1', min_value=60, max_value=80, step=30), \n",
    "             dropout=hp.Float('dropout_1', min_value=0.2, max_value=0.3, step=0.1), \n",
    "             return_sequences=True),\n",
    "        LSTM(hp.Int('lstm_units_2', min_value=30, max_value=60, step=10), \n",
    "             dropout=hp.Float('dropout_2', min_value=0.2, max_value=0.3, step=0.1), \n",
    "             return_sequences=True),\n",
    "        LSTM(hp.Int('lstm_units_3', min_value=10, max_value=30, step=10), \n",
    "             dropout=0.2, \n",
    "             return_sequences=False),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='LSTM_mobileNet_tuning',\n",
    "    project_name='Bay_ischaemia_tuning'  # Updated project name\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
    "\n",
    "# List to store trial results\n",
    "trial_results = []\n",
    "\n",
    "# Custom callback to log each trial with hyperparameters in separate columns\n",
    "class TrialLogger(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        \n",
    "        # Get the current trial's hyperparameters\n",
    "        trial_hyperparams = {key: value for key, value in self.params.items()}\n",
    "        \n",
    "        # Create a trial info dictionary with hyperparameters as separate columns\n",
    "        trial_info = {\n",
    "            'trial_number': len(trial_results) + 1,\n",
    "            'accuracy': logs['accuracy'],\n",
    "            'loss': logs['loss'],\n",
    "            'val_accuracy': logs['val_accuracy'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'training_time': elapsed_time,\n",
    "        }\n",
    "\n",
    "        # Add hyperparameters to the trial_info\n",
    "        for key, value in trial_hyperparams.items():\n",
    "            trial_info[key] = value  # Each hyperparameter gets its own column\n",
    "\n",
    "        trial_results.append(trial_info)\n",
    "\n",
    "# Run the search and log trial results\n",
    "tuner.search(X_train_ischaemia, y_train_ischaemia, epochs=10,  # Updated variable names\n",
    "             validation_data=(X_val_ischaemia, y_val_ischaemia),  # Updated variable names\n",
    "             callbacks=[reduce_lr, early_stopping, TrialLogger()])\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "trial_df = pd.DataFrame(trial_results)\n",
    "\n",
    "# Print the results for each trial\n",
    "print(trial_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84899872-d5a2-4800-bb25-a7b1ab9e8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.6303 - loss: 0.7123 - val_accuracy: 0.6734 - val_loss: 0.5887 - learning_rate: 0.0050\n",
      "Epoch 2/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7226 - loss: 0.5610 - val_accuracy: 0.7378 - val_loss: 0.5525 - learning_rate: 0.0050\n",
      "Epoch 3/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7461 - loss: 0.5263 - val_accuracy: 0.7608 - val_loss: 0.4976 - learning_rate: 0.0050\n",
      "Epoch 4/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7672 - loss: 0.4957 - val_accuracy: 0.7482 - val_loss: 0.5156 - learning_rate: 0.0050\n",
      "Epoch 5/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.7590 - loss: 0.5070 - val_accuracy: 0.7626 - val_loss: 0.4757 - learning_rate: 0.0050\n",
      "Epoch 6/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7758 - loss: 0.4811 - val_accuracy: 0.7721 - val_loss: 0.4791 - learning_rate: 0.0050\n",
      "Epoch 7/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7915 - loss: 0.4595 - val_accuracy: 0.7595 - val_loss: 0.4707 - learning_rate: 0.0050\n",
      "Epoch 8/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7645 - loss: 0.4829 - val_accuracy: 0.7752 - val_loss: 0.4578 - learning_rate: 0.0050\n",
      "Epoch 9/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7941 - loss: 0.4459 - val_accuracy: 0.7275 - val_loss: 0.5536 - learning_rate: 0.0050\n",
      "Epoch 10/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7769 - loss: 0.4556 - val_accuracy: 0.7829 - val_loss: 0.4733 - learning_rate: 0.0050\n",
      "Epoch 11/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7932 - loss: 0.4448 - val_accuracy: 0.7914 - val_loss: 0.4375 - learning_rate: 1.0000e-03\n",
      "Epoch 12/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8171 - loss: 0.4202 - val_accuracy: 0.7914 - val_loss: 0.4396 - learning_rate: 1.0000e-03\n",
      "Epoch 13/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8182 - loss: 0.4144 - val_accuracy: 0.7959 - val_loss: 0.4208 - learning_rate: 1.0000e-03\n",
      "Epoch 14/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8190 - loss: 0.4055 - val_accuracy: 0.7860 - val_loss: 0.4305 - learning_rate: 1.0000e-03\n",
      "Epoch 15/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8207 - loss: 0.3965 - val_accuracy: 0.7919 - val_loss: 0.4325 - learning_rate: 1.0000e-03\n",
      "Epoch 16/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8221 - loss: 0.3980 - val_accuracy: 0.7955 - val_loss: 0.4310 - learning_rate: 2.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8250 - loss: 0.3959 - val_accuracy: 0.8032 - val_loss: 0.4162 - learning_rate: 2.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8260 - loss: 0.3901 - val_accuracy: 0.7959 - val_loss: 0.4244 - learning_rate: 2.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8322 - loss: 0.3816 - val_accuracy: 0.7955 - val_loss: 0.4192 - learning_rate: 2.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m 41/216\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8457 - loss: 0.3527"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters and build the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Fit the best model\n",
    "history = best_model.fit(X_train_ischaemia, y_train_ischaemia, epochs=20, batch_size=32, \n",
    "                          validation_data=(X_val_ischaemia, y_val_ischaemia), \n",
    "                          callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the best model\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_ischaemia, y_test_ischaemia)\n",
    "\n",
    "# Predictions and classification metrics\n",
    "y_pred = best_model.predict(X_test_ischaemia)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test_ischaemia, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_test_ischaemia, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_test_ischaemia, y_pred_classes, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780082d7-6486-4344-b332-ccd171dba704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store best trial results\n",
    "best_trial = {\n",
    "    'Accuracy (%)': test_accuracy * 100,\n",
    "    'Loss (%)': test_loss * 100,\n",
    "    'Training Time (s)': trial_df['training_time'].iloc[trial_df['val_accuracy'].idxmax()],\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1,\n",
    "}\n",
    "\n",
    "# Display the best trial metrics\n",
    "print(\"\\nBest Trial Results:\")\n",
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3b239-3bfb-4cd1-b494-9153df9a3e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
