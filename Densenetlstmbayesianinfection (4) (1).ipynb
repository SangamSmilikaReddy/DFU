{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564a4d6f-479f-4e9c-807d-9a9b07cee5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.0.0)\n",
      "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.0.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.31.0)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.10.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.1.8)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn numpy pillow keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b667b5-a510-446f-ba8c-58693730a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 20:01:41.279230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ischaemia Class Mapping:\n",
      "non-ischemia: 1\n",
      "ischemia: 0\n",
      "Infection Class Mapping:\n",
      "non-infection: 1\n",
      "infection: 0\n",
      "Shape of Ischaemia images array: (9870, 256, 256, 3)\n",
      "Shape of Ischaemia target labels array: (9870,)\n",
      "Shape of Infection images array: (5890, 256, 256, 3)\n",
      "Shape of Infection target labels array: (5890,)\n",
      "Ischaemia Training set shape: (6909, 256, 256, 3) (6909,)\n",
      "Ischaemia Validation set shape: (2220, 256, 256, 3) (2220,)\n",
      "Ischaemia Test set shape: (741, 256, 256, 3) (741,)\n",
      "Infection Training set shape: (4123, 256, 256, 3) (4123,)\n",
      "Infection Validation set shape: (1325, 256, 256, 3) (1325,)\n",
      "Infection Test set shape: (442, 256, 256, 3) (442,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, LSTM, Dense, Dropout, BatchNormalization, Reshape\n",
    "from keras.applications import DenseNet121\n",
    "from keras.regularizers import l2\n",
    "from keras_tuner import BayesianOptimization, HyperParameters\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner.engine.hypermodel import HyperModel\n",
    "\n",
    "# Define the root directory where your image folders are located\n",
    "root_directory = \"PartB_DFU_dataset - Copy\"\n",
    "\n",
    "# Initialize lists to store image paths and corresponding class labels for both datasets\n",
    "image_paths_ischaemia = []\n",
    "categories_ischaemia = []\n",
    "image_paths_infection = []\n",
    "categories_infection = []\n",
    "\n",
    "# Iterate over each class and its subdirectories\n",
    "for class_name in [\"Infection\", \"Ischaemia\"]:\n",
    "    for augmentation_type in [\"Aug-Negative\", \"Aug-Positive\"]:\n",
    "        folder_path = os.path.join(root_directory, class_name, augmentation_type)\n",
    "        category = f\"{class_name.lower()}{'pov' if 'Positive' in augmentation_type else 'neg'}\"\n",
    "        \n",
    "        # Iterate over image files in the current directory\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".jpg\"):  # Assuming images are jpg format\n",
    "                image_path = os.path.join(folder_path, file_name)\n",
    "                if class_name == \"Ischaemia\":\n",
    "                    image_paths_ischaemia.append(image_path)\n",
    "                    categories_ischaemia.append(\"ischemia\" if \"Positive\" in augmentation_type else \"non-ischemia\")\n",
    "                elif class_name == \"Infection\":\n",
    "                    image_paths_infection.append(image_path)\n",
    "                    categories_infection.append(\"infection\" if \"Positive\" in augmentation_type else \"non-infection\")\n",
    "\n",
    "# Create DataFrames for each dataset\n",
    "df_ischaemia = pd.DataFrame({\"category\": categories_ischaemia, \"image_path\": image_paths_ischaemia})\n",
    "df_infection = pd.DataFrame({\"category\": categories_infection, \"image_path\": image_paths_infection})\n",
    "\n",
    "# Label encoding for Ischaemia dataset\n",
    "label_encoder_ischaemia = LabelEncoder()\n",
    "df_ischaemia['Class_Label'] = label_encoder_ischaemia.fit_transform(df_ischaemia['category'])\n",
    "print(\"Ischaemia Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_ischaemia['category'].unique(), df_ischaemia['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Label encoding for Infection dataset\n",
    "label_encoder_infection = LabelEncoder()\n",
    "df_infection['Class_Label'] = label_encoder_infection.fit_transform(df_infection['category'])\n",
    "print(\"Infection Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_infection['category'].unique(), df_infection['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Shuffle both DataFrames\n",
    "df_ischaemia = df_ischaemia.sample(frac=1).reset_index(drop=True)\n",
    "df_infection = df_infection.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Helper function to load and process images\n",
    "def load_images(df):\n",
    "    images = []\n",
    "    target_labels = []   \n",
    "    for index, row in df.iterrows():\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        image_array = np.array(image.resize((256, 256)))  # Resize image to fit DenseNet input size\n",
    "        images.append(image_array)\n",
    "        target_labels.append(row['Class_Label'])\n",
    "    return np.array(images), np.array(target_labels)\n",
    "\n",
    "# Load images for both datasets\n",
    "images_ischaemia, target_labels_ischaemia = load_images(df_ischaemia)\n",
    "images_infection, target_labels_infection = load_images(df_infection)\n",
    "\n",
    "print(\"Shape of Ischaemia images array:\", images_ischaemia.shape)\n",
    "print(\"Shape of Ischaemia target labels array:\", target_labels_ischaemia.shape)\n",
    "print(\"Shape of Infection images array:\", images_infection.shape)\n",
    "print(\"Shape of Infection target labels array:\", target_labels_infection.shape)\n",
    "\n",
    "# Split the Ischaemia dataset\n",
    "X_train_ischaemia, X_test_ischaemia, y_train_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    images_ischaemia, target_labels_ischaemia, test_size=0.3, random_state=42)\n",
    "X_val_ischaemia, X_test_ischaemia, y_val_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    X_test_ischaemia, y_test_ischaemia, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "# Split the Infection dataset\n",
    "X_train_infection, X_test_infection, y_train_infection, y_test_infection = train_test_split(\n",
    "    images_infection, target_labels_infection, test_size=0.3, random_state=42)\n",
    "X_val_infection, X_test_infection, y_val_infection, y_test_infection = train_test_split(\n",
    "    X_test_infection, y_test_infection, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "print(\"Ischaemia Training set shape:\", X_train_ischaemia.shape, y_train_ischaemia.shape)\n",
    "print(\"Ischaemia Validation set shape:\", X_val_ischaemia.shape, y_val_ischaemia.shape)\n",
    "print(\"Ischaemia Test set shape:\", X_test_ischaemia.shape, y_test_ischaemia.shape)\n",
    "print(\"Infection Training set shape:\", X_train_infection.shape, y_train_infection.shape)\n",
    "print(\"Infection Validation set shape:\", X_val_infection.shape, y_val_infection.shape)\n",
    "print(\"Infection Test set shape:\", X_test_infection.shape, y_test_infection.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e79104-82b5-40ba-b32f-6ff0586673c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo\n",
      "Successfully installed py-cpuinfo-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install py-cpuinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b93d0af-829d-4931-b77d-e9fb98364684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 20:07:13.366719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22066 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:42:00.0, compute capability: 8.9\n",
      "2024-12-21 20:07:13.369230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22066 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:61:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step \n"
     ]
    }
   ],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "        for layer in self.base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential([\n",
    "            self.base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Reshape((8, 128)),\n",
    "            LSTM(\n",
    "                units=hp.Int('units_lstm1', min_value=16, max_value=32, step=8),\n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(hp.Float('l2_lstm1', min_value=0.001, max_value=0.01, step=0.001))\n",
    "            ),\n",
    "            Dropout(hp.Float('dropout_lstm1', min_value=0.4, max_value=0.5, step=0.1)),\n",
    "            LSTM(\n",
    "                units=hp.Int('units_lstm2', min_value=16, max_value=32, step=8),\n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(hp.Float('l2_lstm2', min_value=0.001, max_value=0.01, step=0.001))\n",
    "            ),\n",
    "            Dropout(hp.Float('dropout_lstm2', min_value=0.4, max_value=0.5, step=0.1)),\n",
    "            LSTM(\n",
    "                units=hp.Int('units_lstm3', min_value=16, max_value=32, step=8),\n",
    "                kernel_regularizer=l2(hp.Float('l2_lstm3', min_value=0.001, max_value=0.01, step=0.001))\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dense(\n",
    "                units=hp.Int('units_dense1', min_value=32, max_value=64, step=16),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=l2(hp.Float('l2_dense1', min_value=0.001, max_value=0.01, step=0.001))\n",
    "            ),\n",
    "            Dropout(hp.Float('dropout_dense1', min_value=0.4, max_value=0.5, step=0.1)),\n",
    "            BatchNormalization(),\n",
    "            Dense(\n",
    "                units=hp.Int('units_dense2', min_value=32, max_value=64, step=16),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=l2(hp.Float('l2_dense2', min_value=0.001, max_value=0.01, step=0.001))\n",
    "            ),\n",
    "            Dropout(hp.Float('dropout_dense2', min_value=0.4, max_value=0.5, step=0.1)),\n",
    "            BatchNormalization(),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "# Instantiate the HyperModel\n",
    "hypermodel = MyHyperModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad764582-2abd-49c5-b321-dd5e7a585dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVX2 is supported.\n",
      "FMA is supported.\n"
     ]
    }
   ],
   "source": [
    "import cpuinfo\n",
    "\n",
    "info = cpuinfo.get_cpu_info()\n",
    "features = info.get('flags', [])\n",
    "\n",
    "if 'avx2' in features:\n",
    "    print(\"AVX2 is supported.\")\n",
    "else:\n",
    "    print(\"AVX2 is not supported.\")\n",
    "\n",
    "if 'fma' in features:\n",
    "    print(\"FMA is supported.\")\n",
    "else:\n",
    "    print(\"FMA is not supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e8537ef-b7f2-461f-be79-730563a60285",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='infection_dir1',\n",
    "    project_name='infection_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a104ba-b036-4371-9c43-e6cdc0d29e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 06m 18s]\n",
      "val_accuracy: 0.7433962225914001\n",
      "\n",
      "Best val_accuracy So Far: 0.7547169923782349\n",
      "Total elapsed time: 00h 30m 26s\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "tuner.search(X_train_infection, y_train_infection, epochs=50, validation_data=(X_val_infection, y_val_infection), callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f658e85b-d7f0-4e32-96d8-3d2c1f0c52a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 44 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.7594 - loss: 0.5367\n",
      "Infection Test Accuracy: 0.7556561231613159\n"
     ]
    }
   ],
   "source": [
    "best_model_infection = tuner.get_best_models(num_models=1)[0]\n",
    "test_loss_infection, test_accuracy_infection = best_model_infection.evaluate(X_test_infection, y_test_infection)\n",
    "print(f\"Infection Test Accuracy: {test_accuracy_infection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c5d3a-f746-4544-9e2a-97fdeb5ac4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 92ms/step - accuracy: 0.3874 - loss: 2.2677 - val_accuracy: 0.4853 - val_loss: 1.6892 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 63ms/step - accuracy: 0.4992 - loss: 1.7688 - val_accuracy: 0.5675 - val_loss: 1.4239 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.5808 - loss: 1.4495 - val_accuracy: 0.5909 - val_loss: 1.2796 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.6041 - loss: 1.2837 - val_accuracy: 0.6506 - val_loss: 1.1497 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.6440 - loss: 1.1493 - val_accuracy: 0.6740 - val_loss: 1.0604 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.6605 - loss: 1.0596 - val_accuracy: 0.6845 - val_loss: 0.9907 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m 11/129\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.6867 - loss: 0.9612"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Start time for training\n",
    "start_training_time = time.time()\n",
    "\n",
    "# Get the best hyperparameters and build the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Fit the model and collect the training metrics\n",
    "history = best_model.fit(X_train_infection, y_train_infection, epochs=20, batch_size=32, \n",
    "                         validation_data=(X_val_infection, y_val_infection), \n",
    "                         callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Collect training metrics\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "train_f1 = f1_score(y_train_infection, np.argmax(best_model.predict(X_train_infection), axis=1), average='macro')\n",
    "train_precision = precision_score(y_train_infection, np.argmax(best_model.predict(X_train_infection), axis=1), average='macro')\n",
    "train_recall = recall_score(y_train_infection, np.argmax(best_model.predict(X_train_infection), axis=1), average='macro')\n",
    "\n",
    "# End time for training\n",
    "end_training_time = time.time()\n",
    "training_time = end_training_time - start_training_time\n",
    "\n",
    "# Start time for validation\n",
    "start_validation_time = time.time()\n",
    "\n",
    "# Validate the model\n",
    "val_loss, val_accuracy = best_model.evaluate(X_val_infection, y_val_infection)\n",
    "\n",
    "# Collect validation metrics\n",
    "val_f1 = f1_score(y_val_infection, np.argmax(best_model.predict(X_val_infection), axis=1), average='macro')\n",
    "val_precision = precision_score(y_val_infection, np.argmax(best_model.predict(X_val_infection), axis=1), average='macro')\n",
    "val_recall = recall_score(y_val_infection, np.argmax(best_model.predict(X_val_infection), axis=1), average='macro')\n",
    "\n",
    "# End time for validation\n",
    "end_validation_time = time.time()\n",
    "validation_time = end_validation_time - start_validation_time\n",
    "\n",
    "# Start time for testing\n",
    "start_testing_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_infection, y_test_infection)\n",
    "\n",
    "# Predictions for classification metrics\n",
    "y_pred = best_model.predict(X_test_infection)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Collect testing metrics\n",
    "test_f1 = f1_score(y_test_infection, y_pred_classes, average='macro')\n",
    "test_precision = precision_score(y_test_infection, y_pred_classes, average='macro')\n",
    "test_recall = recall_score(y_test_infection, y_pred_classes, average='macro')\n",
    "\n",
    "# End time for testing\n",
    "end_testing_time = time.time()\n",
    "testing_time = end_testing_time - start_testing_time\n",
    "\n",
    "# Collect all metrics in a dictionary for easy access\n",
    "metrics = {\n",
    "    'Training Accuracy': train_accuracy,\n",
    "    'Training Loss': train_loss,\n",
    "    'Training F1 Score': train_f1,\n",
    "    'Training Precision': train_precision,\n",
    "    'Training Recall': train_recall,\n",
    "    'Training Time (s)': training_time,\n",
    "    'Validation Accuracy': val_accuracy,\n",
    "    'Validation Loss': val_loss,\n",
    "    'Validation F1 Score': val_f1,\n",
    "    'Validation Precision': val_precision,\n",
    "    'Validation Recall': val_recall,\n",
    "    'Validation Time (s)': validation_time,\n",
    "    'Test Accuracy': test_accuracy,\n",
    "    'Test F1 Score': test_f1,\n",
    "    'Test Precision': test_precision,\n",
    "    'Test Recall': test_recall,\n",
    "    'Testing Time (s)': testing_time\n",
    "}\n",
    "\n",
    "# Print the collected metrics\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6060b-79a8-4bc7-ba16-e7515cb50671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
