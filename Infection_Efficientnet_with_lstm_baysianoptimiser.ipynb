{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46002e6-eaad-47a9-8292-2188203ece6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed successfully to 'DFU_dataset'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile as zf\n",
    "\n",
    "# Path to the repaired ZIP file\n",
    "zip_file_path = \"PartB_DFU_dataset - Copy.zip\"\n",
    "extract_path = \"DFU_dataset\"\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    try:\n",
    "        with zf.ZipFile(zip_file_path, 'r') as files:\n",
    "            files.extractall(extract_path)\n",
    "        print(f\"Extraction completed successfully to '{extract_path}'\")\n",
    "    except zf.BadZipFile:\n",
    "        print(\"Error: The ZIP file is corrupted.\")\n",
    "    except OSError as e:\n",
    "        print(f\"OS error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"Error: The file '{zip_file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84251448-be88-4fd5-bc82-10122731381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ischaemia Class Mapping:\n",
      "non-ischemia: 1\n",
      "ischemia: 0\n",
      "Infection Class Mapping:\n",
      "non-infection: 1\n",
      "infection: 0\n",
      "Shape of Ischaemia images array: (9870, 224, 224, 3)\n",
      "Shape of Ischaemia target labels array: (9870,)\n",
      "Shape of Infection images array: (5890, 224, 224, 3)\n",
      "Shape of Infection target labels array: (5890,)\n",
      "Ischaemia Training set shape: (6909, 224, 224, 3) (6909,)\n",
      "Ischaemia Validation set shape: (2220, 224, 224, 3) (2220,)\n",
      "Ischaemia Test set shape: (741, 224, 224, 3) (741,)\n",
      "Infection Training set shape: (4123, 224, 224, 3) (4123,)\n",
      "Infection Validation set shape: (1325, 224, 224, 3) (1325,)\n",
      "Infection Test set shape: (442, 224, 224, 3) (442,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Define the root directory where your image folders are located\n",
    "root_directory = \"DFU_dataset/PartB_DFU_dataset - Copy\"\n",
    "\n",
    "# Initialize lists to store image paths and corresponding class labels for both datasets\n",
    "image_paths_ischaemia = []\n",
    "categories_ischaemia = []\n",
    "image_paths_infection = []\n",
    "categories_infection = []\n",
    "\n",
    "# Iterate over each class and its subdirectories\n",
    "for class_name in [\"Infection\", \"Ischaemia\"]:\n",
    "    for augmentation_type in [\"Aug-Negative\", \"Aug-Positive\"]:\n",
    "        folder_path = os.path.join(root_directory, class_name, augmentation_type)\n",
    "        category = f\"{class_name.lower()}{'pov' if 'Positive' in augmentation_type else 'neg'}\"\n",
    "        \n",
    "        # Iterate over image files in the current directory\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".jpg\"):  # Assuming images are jpg format\n",
    "                image_path = os.path.join(folder_path, file_name)\n",
    "                if class_name == \"Ischaemia\":\n",
    "                    image_paths_ischaemia.append(image_path)\n",
    "                    categories_ischaemia.append(\"ischemia\" if \"Positive\" in augmentation_type else \"non-ischemia\")\n",
    "                elif class_name == \"Infection\":\n",
    "                    image_paths_infection.append(image_path)\n",
    "                    categories_infection.append(\"infection\" if \"Positive\" in augmentation_type else \"non-infection\")\n",
    "\n",
    "# Create DataFrames for each dataset\n",
    "df_ischaemia = pd.DataFrame({\"category\": categories_ischaemia, \"image_path\": image_paths_ischaemia})\n",
    "df_infection = pd.DataFrame({\"category\": categories_infection, \"image_path\": image_paths_infection})\n",
    "\n",
    "# Label encoding for Ischaemia dataset\n",
    "label_encoder_ischaemia = LabelEncoder()\n",
    "df_ischaemia['Class_Label'] = label_encoder_ischaemia.fit_transform(df_ischaemia['category'])\n",
    "print(\"Ischaemia Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_ischaemia['category'].unique(), df_ischaemia['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Label encoding for Infection dataset\n",
    "label_encoder_infection = LabelEncoder()\n",
    "df_infection['Class_Label'] = label_encoder_infection.fit_transform(df_infection['category'])\n",
    "print(\"Infection Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_infection['category'].unique(), df_infection['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Shuffle both DataFrames\n",
    "df_ischaemia = df_ischaemia.sample(frac=1).reset_index(drop=True)\n",
    "df_infection = df_infection.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Helper function to load and process images\n",
    "def load_images(df):\n",
    "    images = []\n",
    "    target_labels = []   \n",
    "    for index, row in df.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        image_array = np.array(image.resize((224, 224)))  # Resize image to fit MobileNet input size\n",
    "        images.append(image_array)\n",
    "        target_labels.append(row['Class_Label'])\n",
    "    return np.array(images), np.array(target_labels)\n",
    "\n",
    "# Load images for both datasets\n",
    "images_ischaemia, target_labels_ischaemia = load_images(df_ischaemia)\n",
    "images_infection, target_labels_infection = load_images(df_infection)\n",
    "\n",
    "print(\"Shape of Ischaemia images array:\", images_ischaemia.shape)\n",
    "print(\"Shape of Ischaemia target labels array:\", target_labels_ischaemia.shape)\n",
    "print(\"Shape of Infection images array:\", images_infection.shape)\n",
    "print(\"Shape of Infection target labels array:\", target_labels_infection.shape)\n",
    "\n",
    "# Split the Ischaemia dataset\n",
    "X_train_ischaemia, X_test_ischaemia, y_train_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    images_ischaemia, target_labels_ischaemia, test_size=0.3, random_state=42)\n",
    "X_val_ischaemia, X_test_ischaemia, y_val_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    X_test_ischaemia, y_test_ischaemia, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "# Split the Infection dataset\n",
    "X_train_infection, X_test_infection, y_train_infection, y_test_infection = train_test_split(\n",
    "    images_infection, target_labels_infection, test_size=0.3, random_state=42)\n",
    "X_val_infection, X_test_infection, y_val_infection, y_test_infection = train_test_split(\n",
    "    X_test_infection, y_test_infection, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "print(\"Ischaemia Training set shape:\", X_train_ischaemia.shape, y_train_ischaemia.shape)\n",
    "print(\"Ischaemia Validation set shape:\", X_val_ischaemia.shape, y_val_ischaemia.shape)\n",
    "print(\"Ischaemia Test set shape:\", X_test_ischaemia.shape, y_test_ischaemia.shape)\n",
    "print(\"Infection Training set shape:\", X_train_infection.shape, y_train_infection.shape)\n",
    "print(\"Infection Validation set shape:\", X_val_infection.shape, y_val_infection.shape)\n",
    "print(\"Infection Test set shape:\", X_test_infection.shape, y_test_infection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1543374a-8801-4ec1-a6a0-405ce790e34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 30s]\n",
      "val_accuracy: 0.7781131863594055\n",
      "\n",
      "Best val_accuracy So Far: 0.7781131863594055\n",
      "Total elapsed time: 00h 08m 02s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from kerastuner import HyperModel, HyperParameters\n",
    "from kerastuner.tuners import BayesianOptimization, GridSearch\n",
    "from tensorflow.keras.applications import EfficientNetB0,InceptionV3\n",
    "from tensorflow.keras.layers import Dropout, TimeDistributed, Flatten, LSTM, Dense, BatchNormalization, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    base_model = EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        TimeDistributed(Flatten()),\n",
    "        LSTM(hp.Int('lstm_units_1', min_value=60, max_value=80, step=30), \n",
    "             dropout=hp.Float('dropout_1', min_value=0.2, max_value=0.3, step=0.1), \n",
    "             return_sequences=True),\n",
    "        LSTM(hp.Int('lstm_units_2', min_value=30, max_value=60, step=10), \n",
    "             dropout=hp.Float('dropout_2', min_value=0.2, max_value=0.3, step=0.1), \n",
    "             return_sequences=True),\n",
    "        LSTM(hp.Int('lstm_units_3', min_value=10, max_value=30, step=10), \n",
    "             dropout=0.2, \n",
    "             return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='LSTM_effB0_tunning',\n",
    "    project_name='Bay_infec_tuning'\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
    "\n",
    "tuner.search(X_train_infection, y_train_infection, epochs=20, validation_data=(X_val_infection, y_val_infection), callbacks=[reduce_lr, early_stopping])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84899872-d5a2-4800-bb25-a7b1ab9e8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 86ms/step - accuracy: 0.5251 - loss: 0.9705 - val_accuracy: 0.5691 - val_loss: 0.7187 - learning_rate: 0.0050\n",
      "Epoch 2/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.6531 - loss: 0.6363 - val_accuracy: 0.7283 - val_loss: 0.5809 - learning_rate: 0.0050\n",
      "Epoch 3/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.7120 - loss: 0.5701 - val_accuracy: 0.7389 - val_loss: 0.5564 - learning_rate: 0.0050\n",
      "Epoch 4/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.7460 - loss: 0.5287 - val_accuracy: 0.7268 - val_loss: 0.5517 - learning_rate: 0.0050\n",
      "Epoch 5/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.7735 - loss: 0.5047 - val_accuracy: 0.7328 - val_loss: 0.5377 - learning_rate: 0.0050\n",
      "Epoch 6/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.7791 - loss: 0.4710 - val_accuracy: 0.7147 - val_loss: 0.5968 - learning_rate: 0.0050\n",
      "Epoch 7/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.7649 - loss: 0.4930 - val_accuracy: 0.7570 - val_loss: 0.5747 - learning_rate: 0.0050\n",
      "Epoch 8/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.8030 - loss: 0.4411 - val_accuracy: 0.7570 - val_loss: 0.5280 - learning_rate: 1.0000e-03\n",
      "Epoch 9/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8199 - loss: 0.4030 - val_accuracy: 0.7691 - val_loss: 0.5164 - learning_rate: 1.0000e-03\n",
      "Epoch 10/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.8322 - loss: 0.3831 - val_accuracy: 0.7683 - val_loss: 0.5037 - learning_rate: 1.0000e-03\n",
      "Epoch 11/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.8348 - loss: 0.3696 - val_accuracy: 0.7592 - val_loss: 0.5332 - learning_rate: 1.0000e-03\n",
      "Epoch 12/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.8473 - loss: 0.3546 - val_accuracy: 0.7623 - val_loss: 0.5418 - learning_rate: 1.0000e-03\n",
      "Epoch 13/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.8588 - loss: 0.3301 - val_accuracy: 0.7706 - val_loss: 0.5272 - learning_rate: 2.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.8750 - loss: 0.3039 - val_accuracy: 0.7796 - val_loss: 0.5243 - learning_rate: 2.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.8751 - loss: 0.3031 - val_accuracy: 0.7864 - val_loss: 0.5119 - learning_rate: 4.0000e-05\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8865 - loss: 0.2668\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7828 - loss: 0.5268\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7602 - loss: 0.5365\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step\n",
      "Training Accuracy: 0.892553985118866\n",
      "Training Loss: 0.25308629870414734\n",
      "Validation Accuracy: 0.7758490443229675\n",
      "Validation Loss: 0.5402886867523193\n",
      "Test Accuracy: 0.892553985118866\n",
      "Precision: 0.7828719008264462\n",
      "Recall: 0.7826324807497781\n",
      "F1 Score: 0.7760169530566177\n"
     ]
    }
   ],
   "source": [
    "#history = model.fit(X_train_infection, y_train_infection, epochs=30, batch_size=64, validation_data=(X_val_infection, y_val_infection), callbacks=[early_stopping, reduce_lr])\n",
    "test_loss,test_accuracy = model.evaluate(X_test_infection, y_test_infection)\n",
    "y_pred = model.predict([X_test_infection, y_test_infection])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "precision = precision_score(y_test_infection, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_test_infection, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_test_infection, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"Test Accuracy:\", train_accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa122a9c-0698-4e01-a93d-cc5b9976731b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
