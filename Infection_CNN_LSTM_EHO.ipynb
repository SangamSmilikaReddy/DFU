{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576589cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed successfully to 'DFU_dataset'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile as zf\n",
    "\n",
    "# Path to the repaired ZIP file\n",
    "zip_file_path = \"PartB_DFU_dataset - Copy.zip\"\n",
    "extract_path = \"DFU_dataset\"\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    try:\n",
    "        with zf.ZipFile(zip_file_path, 'r') as files:\n",
    "            files.extractall(extract_path)\n",
    "        print(f\"Extraction completed successfully to '{extract_path}'\")\n",
    "    except zf.BadZipFile:\n",
    "        print(\"Error: The ZIP file is corrupted.\")\n",
    "    except OSError as e:\n",
    "        print(f\"OS error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"Error: The file '{zip_file_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010d9458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ischaemia Class Mapping:\n",
      "non-ischemia: 1\n",
      "ischemia: 0\n",
      "Infection Class Mapping:\n",
      "non-infection: 1\n",
      "infection: 0\n",
      "Shape of Ischaemia images array: (9870, 224, 224, 3)\n",
      "Shape of Ischaemia target labels array: (9870,)\n",
      "Shape of Infection images array: (5890, 224, 224, 3)\n",
      "Shape of Infection target labels array: (5890,)\n",
      "Ischaemia Training set shape: (6909, 224, 224, 3) (6909,)\n",
      "Ischaemia Validation set shape: (2220, 224, 224, 3) (2220,)\n",
      "Ischaemia Test set shape: (741, 224, 224, 3) (741,)\n",
      "Infection Training set shape: (4123, 224, 224, 3) (4123,)\n",
      "Infection Validation set shape: (1325, 224, 224, 3) (1325,)\n",
      "Infection Test set shape: (442, 224, 224, 3) (442,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Define the root directory where your image folders are located\n",
    "root_directory = \"/DFU_dataset/PartB_DFU_dataset - Copy\"\n",
    "\n",
    "# Initialize lists to store image paths and corresponding class labels for both datasets\n",
    "image_paths_ischaemia = []\n",
    "categories_ischaemia = []\n",
    "image_paths_infection = []\n",
    "categories_infection = []\n",
    "\n",
    "# Iterate over each class and its subdirectories\n",
    "for class_name in [\"Infection\", \"Ischaemia\"]:\n",
    "    for augmentation_type in [\"Aug-Negative\", \"Aug-Positive\"]:\n",
    "        folder_path = os.path.join(root_directory, class_name, augmentation_type)\n",
    "        category = f\"{class_name.lower()}{'pov' if 'Positive' in augmentation_type else 'neg'}\"\n",
    "        \n",
    "        # Iterate over image files in the current directory\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".jpg\"):  # Assuming images are jpg format\n",
    "                image_path = os.path.join(folder_path, file_name)\n",
    "                if class_name == \"Ischaemia\":\n",
    "                    image_paths_ischaemia.append(image_path)\n",
    "                    categories_ischaemia.append(\"ischemia\" if \"Positive\" in augmentation_type else \"non-ischemia\")\n",
    "                elif class_name == \"Infection\":\n",
    "                    image_paths_infection.append(image_path)\n",
    "                    categories_infection.append(\"infection\" if \"Positive\" in augmentation_type else \"non-infection\")\n",
    "\n",
    "# Create DataFrames for each dataset\n",
    "df_ischaemia = pd.DataFrame({\"category\": categories_ischaemia, \"image_path\": image_paths_ischaemia})\n",
    "df_infection = pd.DataFrame({\"category\": categories_infection, \"image_path\": image_paths_infection})\n",
    "\n",
    "# Label encoding for Ischaemia dataset\n",
    "label_encoder_ischaemia = LabelEncoder()\n",
    "df_ischaemia['Class_Label'] = label_encoder_ischaemia.fit_transform(df_ischaemia['category'])\n",
    "print(\"Ischaemia Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_ischaemia['category'].unique(), df_ischaemia['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Label encoding for Infection dataset\n",
    "label_encoder_infection = LabelEncoder()\n",
    "df_infection['Class_Label'] = label_encoder_infection.fit_transform(df_infection['category'])\n",
    "print(\"Infection Class Mapping:\")\n",
    "for class_label, numerical_label in zip(df_infection['category'].unique(), df_infection['Class_Label'].unique()):\n",
    "    print(f\"{class_label}: {numerical_label}\")\n",
    "\n",
    "# Shuffle both DataFrames\n",
    "df_ischaemia = df_ischaemia.sample(frac=1).reset_index(drop=True)\n",
    "df_infection = df_infection.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Helper function to load and process images\n",
    "def load_images(df):\n",
    "    images = []\n",
    "    target_labels = []   \n",
    "    for index, row in df.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        image_array = np.array(image.resize((224, 224)))  # Resize image to fit MobileNet input size\n",
    "        images.append(image_array)\n",
    "        target_labels.append(row['Class_Label'])\n",
    "    return np.array(images), np.array(target_labels)\n",
    "\n",
    "# Load images for both datasets\n",
    "images_ischaemia, target_labels_ischaemia = load_images(df_ischaemia)\n",
    "images_infection, target_labels_infection = load_images(df_infection)\n",
    "\n",
    "print(\"Shape of Ischaemia images array:\", images_ischaemia.shape)\n",
    "print(\"Shape of Ischaemia target labels array:\", target_labels_ischaemia.shape)\n",
    "print(\"Shape of Infection images array:\", images_infection.shape)\n",
    "print(\"Shape of Infection target labels array:\", target_labels_infection.shape)\n",
    "\n",
    "# Split the Ischaemia dataset\n",
    "X_train_ischaemia, X_test_ischaemia, y_train_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    images_ischaemia, target_labels_ischaemia, test_size=0.3, random_state=42)\n",
    "X_val_ischaemia, X_test_ischaemia, y_val_ischaemia, y_test_ischaemia = train_test_split(\n",
    "    X_test_ischaemia, y_test_ischaemia, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "# Split the Infection dataset\n",
    "X_train_infection, X_test_infection, y_train_infection, y_test_infection = train_test_split(\n",
    "    images_infection, target_labels_infection, test_size=0.3, random_state=42)\n",
    "X_val_infection, X_test_infection, y_val_infection, y_test_infection = train_test_split(\n",
    "    X_test_infection, y_test_infection, test_size=0.25, random_state=42)  # 0.25 * 0.3 = 0.075\n",
    "\n",
    "print(\"Ischaemia Training set shape:\", X_train_ischaemia.shape, y_train_ischaemia.shape)\n",
    "print(\"Ischaemia Validation set shape:\", X_val_ischaemia.shape, y_val_ischaemia.shape)\n",
    "print(\"Ischaemia Test set shape:\", X_test_ischaemia.shape, y_test_ischaemia.shape)\n",
    "print(\"Infection Training set shape:\", X_train_infection.shape, y_train_infection.shape)\n",
    "print(\"Infection Validation set shape:\", X_val_infection.shape, y_val_infection.shape)\n",
    "print(\"Infection Test set shape:\", X_test_infection.shape, y_test_infection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e28c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# ELEPHANT HERDING OPTIMIZATION (EHO) Algorithm Implementation\n",
    "############################################################################\n",
    "\n",
    "def initial_variables(size, min_values, max_values, target_function, start_init=None):\n",
    "    dim = len(min_values)\n",
    "    \n",
    "    if start_init is not None:\n",
    "        start_init = np.atleast_2d(start_init)\n",
    "        n_rows = size - start_init.shape[0]\n",
    "        if n_rows > 0:\n",
    "            rows = np.random.uniform(min_values, max_values, (n_rows, dim))\n",
    "            start_init = np.vstack((start_init[:, :dim], rows))\n",
    "        else:\n",
    "            start_init = start_init[:size, :dim]\n",
    "        \n",
    "        fitness_values = np.array([target_function(ind) for ind in start_init])\n",
    "        # Ensure fitness_values is a 2D array for concatenation\n",
    "        if fitness_values.ndim == 1:\n",
    "            fitness_values = fitness_values.reshape(-1, 1)\n",
    "        print(f\"Shape of start_init: {start_init.shape}\")\n",
    "        print(f\"Shape of fitness_values: {fitness_values.shape}\")\n",
    "        population = np.hstack((start_init, fitness_values))\n",
    "    else:\n",
    "        population = np.random.uniform(min_values, max_values, (size, dim))\n",
    "        fitness_values = np.array([target_function(ind) for ind in population])\n",
    "        # Ensure fitness_values is a 2D array for concatenation\n",
    "        if fitness_values.ndim == 1:\n",
    "            fitness_values = fitness_values.reshape(-1, 1)\n",
    "        print(f\"Shape of population: {population.shape}\")\n",
    "        print(f\"Shape of fitness_values: {fitness_values.shape}\")\n",
    "        population = np.hstack((population, fitness_values))\n",
    "    \n",
    "    return population\n",
    "\n",
    "\n",
    "\n",
    "def update_herd(population, alpha, beta, best_elephant, idx_b, idx_w, min_values, max_values, target_function):\n",
    "    old_population = np.copy(population)\n",
    "    cut = population.shape[0]\n",
    "    dim = len(min_values)  # Number of hyperparameters (should be 8 in this case)\n",
    "    \n",
    "    for i in range(0, cut):\n",
    "        if i != idx_b and i != idx_w:\n",
    "            r = np.random.rand(dim)\n",
    "            # Ensure best_elephant includes all hyperparameters\n",
    "            population[i, :-1] = np.clip(\n",
    "                old_population[i, :-1] + alpha * (best_elephant[:-1] - old_population[i, :-1]) * r,\n",
    "                min_values, max_values\n",
    "            )\n",
    "        elif i == idx_b:\n",
    "            center = np.mean(old_population[:, :-1], axis=0)\n",
    "            population[i, :-1] = np.clip(beta * center, min_values, max_values)\n",
    "        elif i == idx_w:\n",
    "            random_values = np.random.rand(dim)\n",
    "            population[i, :-1] = np.clip(min_values + (max_values - min_values) * random_values, min_values, max_values)\n",
    "    \n",
    "    # Calculate fitness values\n",
    "    fitness_values = np.array([target_function(ind) for ind in population[:, :-1]])\n",
    "    population[:, -1] = fitness_values\n",
    "    \n",
    "    # Update best and worst indices\n",
    "    idx_b = np.argmin(population[:, -1])\n",
    "    idx_w = np.argmax(population[:, -1])\n",
    "    \n",
    "    if population[idx_b, -1] < best_elephant[-1]:\n",
    "        best_elephant = np.copy(population[idx_b, :])\n",
    "    \n",
    "    return population, best_elephant, idx_b, idx_w\n",
    "\n",
    "\n",
    "\n",
    "def elephant_herding_optimization(size=50, alpha=0.5, beta=0.1, min_values=[0.0001], max_values=[0.01], generations=5000, target_function=target_function, verbose=True, start_init=None, target_value=None):\n",
    "    population = initial_variables(size, min_values, max_values, target_function, start_init)\n",
    "    idx_b = np.argmin(population[:, -1])\n",
    "    idx_w = np.argmax(population[:, -1])\n",
    "    best_elephant = population[idx_b, :]  # Include the entire individual (all hyperparameters)\n",
    "    min_values = np.array(min_values)\n",
    "    max_values = np.array(max_values)\n",
    "    count = 0\n",
    "    while count <= generations:\n",
    "        if verbose:    \n",
    "            print('Generation: ', count, ' f(x) = ', best_elephant[-1])\n",
    "        population, best_elephant, idx_b, idx_w = update_herd(population, alpha, beta, best_elephant, idx_b, idx_w, min_values, max_values, target_function)\n",
    "        if target_value is not None:\n",
    "            if best_elephant[-1] <= target_value:\n",
    "                count = 2 * generations\n",
    "            else:\n",
    "                count = count + 1\n",
    "        else:\n",
    "            count = count + 1\n",
    "    return best_elephant\n",
    "\n",
    "\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00efffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of population: (3, 8)\n",
      "Shape of fitness_values: (3, 1)\n",
      "[[7.55866686e-03 5.05632314e+01 6.12539408e+01 2.39057388e-01\n",
      "  4.95214258e+01 2.80865622e-01 1.55249436e+01 2.73470062e-01\n",
      "  6.93079233e-01]\n",
      " [5.61278456e-03 8.41213269e+00 7.52228400e+01 2.37492563e-01\n",
      "  4.11783727e+01 2.17614787e-01 2.51883962e+01 2.21005754e-01\n",
      "  6.92924440e-01]\n",
      " [8.84954488e-03 3.16177382e+01 7.75721042e+01 2.92877176e-01\n",
      "  4.04269162e+01 2.57135341e-01 1.49427794e+01 2.38826272e-01\n",
      "  6.92991734e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization, GlobalAveragePooling2D, LSTM, TimeDistributed, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the CNN-LSTM model-building function\n",
    "def build_cnn_model(hp): \n",
    "    model = Sequential([\n",
    "        TimeDistributed(Flatten()),\n",
    "        LSTM(hp['lstm_units_1'], \n",
    "             dropout=hp['dropout_1'], \n",
    "             return_sequences=True),\n",
    "        LSTM(hp['lstm_units_2'], \n",
    "             dropout=hp['dropout_2'], \n",
    "             return_sequences=True),\n",
    "        LSTM(hp['lstm_units_3'], \n",
    "             dropout=hp['dropout_3'], \n",
    "             return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(2, activation='softmax')  # Assuming 2 classes for classification\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp['learning_rate']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Modify target_function to include LSTM hyperparameters\n",
    "def target_function(hyperparams):\n",
    "    # Extract learning rate, batch size, LSTM units, and dropout rates from hyperparams\n",
    "    learning_rate = hyperparams[0]\n",
    "    batch_size = int(hyperparams[1])\n",
    "    \n",
    "    lstm_units_1 = int(hyperparams[2])\n",
    "    dropout_1 = float(hyperparams[3])\n",
    "    lstm_units_2 = int(hyperparams[4])\n",
    "    dropout_2 = float(hyperparams[5])\n",
    "    lstm_units_3 = int(hyperparams[6])\n",
    "    dropout_3 = float(hyperparams[7])\n",
    "\n",
    "    # Create a hyperparameter dictionary to pass to the model\n",
    "    hp = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'lstm_units_1': lstm_units_1,\n",
    "        'dropout_1': dropout_1,\n",
    "        'lstm_units_2': lstm_units_2,\n",
    "        'dropout_2': dropout_2,\n",
    "        'lstm_units_3': lstm_units_3,\n",
    "        'dropout_3': dropout_3\n",
    "    }\n",
    "    \n",
    "    # Build the CNN-LSTM model\n",
    "    model = build_cnn_model(hp)\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
    "    \n",
    "    # Train model with the given batch size and hyperparameters\n",
    "    history = model.fit(\n",
    "        X_train_infection, y_train_infection,\n",
    "        batch_size=batch_size,  \n",
    "        epochs=10,\n",
    "        validation_data=(X_val_infection, y_val_infection),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Return the validation loss as the evaluation metric\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss\n",
    "\n",
    "# Define the ranges for learning rate, batch size, and LSTM hyperparameters\n",
    "min_values = [0.0001, 8, 60, 0.2, 30, 0.2, 10, 0.2]  # min values for hyperparameters\n",
    "max_values = [0.01, 128, 80, 0.3, 60, 0.3, 30, 0.3]  # max values for hyperparameters\n",
    "\n",
    "# Now you can use the EHO algorithm as before to optimize these hyperparameters\n",
    "population = initial_variables(3, min_values, max_values, target_function)\n",
    "print(population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b532b6fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of population: (5, 8)\n",
      "Shape of fitness_values: (5, 1)\n",
      "Generation:  0  f(x) =  0.6927728056907654\n",
      "Generation:  1  f(x) =  0.6926170587539673\n",
      "Generation:  2  f(x) =  0.6925433278083801\n",
      "Generation:  3  f(x) =  0.6925433278083801\n",
      "Generation:  4  f(x) =  0.6925433278083801\n",
      "Generation:  5  f(x) =  0.6924322843551636\n",
      "Best Hyperparameters:  [1.41485029e-03 1.69785818e+01 6.10837723e+01 2.35098391e-01\n",
      " 3.33160492e+01 2.20170933e-01 1.06525346e+01 2.25966896e-01\n",
      " 6.92432284e-01]\n"
     ]
    }
   ],
   "source": [
    "# Run Elephant Herding Optimization (EHO)\n",
    "best_params = elephant_herding_optimization(\n",
    "    size=5,  # Number of elephants\n",
    "    alpha=0.5, beta=0.1, \n",
    "    min_values=min_values, max_values=max_values,\n",
    "    generations=5,  # Number of generations\n",
    "    target_function=target_function\n",
    ")\n",
    "\n",
    "# Print the best found hyperparameters\n",
    "print(\"Best Hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af8cd09-f446-4af2-858b-416703499ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 49ms/step - accuracy: 0.4843 - loss: 0.8296 - val_accuracy: 0.5057 - val_loss: 0.6928 - learning_rate: 0.0014\n",
      "Epoch 2/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.4964 - loss: 0.7282 - val_accuracy: 0.4657 - val_loss: 0.6967 - learning_rate: 0.0014\n",
      "Epoch 3/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.5080 - loss: 0.7084 - val_accuracy: 0.5087 - val_loss: 0.6931 - learning_rate: 0.0014\n",
      "Epoch 4/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.4782 - loss: 0.7079 - val_accuracy: 0.4777 - val_loss: 0.6948 - learning_rate: 2.8297e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.4961 - loss: 0.7003 - val_accuracy: 0.5034 - val_loss: 0.6925 - learning_rate: 2.8297e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.4957 - loss: 0.7010 - val_accuracy: 0.4943 - val_loss: 0.6935 - learning_rate: 2.8297e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.4889 - loss: 0.7000 - val_accuracy: 0.4936 - val_loss: 0.6927 - learning_rate: 2.8297e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.4989 - loss: 0.6970 - val_accuracy: 0.4966 - val_loss: 0.6932 - learning_rate: 5.6594e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.4936 - loss: 0.6992 - val_accuracy: 0.4868 - val_loss: 0.6942 - learning_rate: 5.6594e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.5280 - loss: 0.6923 - val_accuracy: 0.5072 - val_loss: 0.6933 - learning_rate: 1.1319e-05\n",
      "Training Accuracy:  0.5137035846710205\n",
      "Training Loss:  0.6947540640830994\n",
      "Validation Accuracy:  0.507169783115387\n",
      "Validation Loss:  0.6932565569877625\n"
     ]
    }
   ],
   "source": [
    "# Extract the best hyperparameters from the EHO result\n",
    "best_learning_rate = best_params[0]  # First parameter: learning rate\n",
    "best_batch_size = int(best_params[1])  # Second parameter: batch size (convert to int)\n",
    "\n",
    "# Extract LSTM hyperparameters from best_params\n",
    "best_lstm_units_1 = int(best_params[2])  # LSTM layer 1 units\n",
    "best_dropout_1 = float(best_params[3])   # LSTM layer 1 dropout\n",
    "best_lstm_units_2 = int(best_params[4])  # LSTM layer 2 units\n",
    "best_dropout_2 = float(best_params[5])   # LSTM layer 2 dropout\n",
    "best_lstm_units_3 = int(best_params[6])  # LSTM layer 3 units\n",
    "best_dropout_3 = float(best_params[7])   # LSTM layer 3 dropout\n",
    "\n",
    "# Create a dictionary for the best hyperparameters\n",
    "hp_best = {\n",
    "    'learning_rate': best_learning_rate,\n",
    "    'lstm_units_1': best_lstm_units_1,\n",
    "    'dropout_1': best_dropout_1,\n",
    "    'lstm_units_2': best_lstm_units_2,\n",
    "    'dropout_2': best_dropout_2,\n",
    "    'lstm_units_3': best_lstm_units_3,\n",
    "    'dropout_3': best_dropout_3\n",
    "}\n",
    "\n",
    "# Build the CNN-LSTM model using the best hyperparameters\n",
    "model = build_cnn_model(hp_best)\n",
    "\n",
    "# Set up callbacks for early stopping and learning rate reduction\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "history = model.fit(\n",
    "    X_train_infection, y_train_infection,\n",
    "    epochs=20,  # You can adjust the number of epochs as needed\n",
    "    batch_size=best_batch_size,\n",
    "    validation_data=(X_val_infection, y_val_infection),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1  # Set to 1 to show training progress\n",
    ")\n",
    "\n",
    "# Get the metrics from the training process\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Accuracy: \", train_acc)\n",
    "print(\"Training Loss: \", train_loss)\n",
    "print(\"Validation Accuracy: \", val_acc)\n",
    "print(\"Validation Loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fa2a2fc-e0c5-4824-b664-84bed89736d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5455 - loss: 0.6920\n",
      "Test Accuracy:  0.529411792755127\n",
      "Test Loss:  0.6927269101142883\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_infection, y_test_infection, verbose=1)\n",
    "\n",
    "# Print the test accuracy and loss\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "print(\"Test Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9c6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4969878,
     "sourceId": 8362139,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
